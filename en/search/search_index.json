{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to FCLib \u70b9\u51fb \u8fd9\u91cc \u67e5\u770b\u4e2d\u6587\u8bf4\u660e\u6587\u6863\u3002 FCLib (fucking cool library) is a C++ library for RGB-D based SLAM and 3D vision. This library is mainly a summary of some basic knowledge and is used to satisfy the author's obsessive-compulsive disorder. It tries to be designed to be relatively simple, clear in structure, easy to understand, and friendly to novices, but limited to the level of the author, there may still be many areas where the design is not good enough. Currently this library does not use GPU except for the visualization part. The author hopes that FCLib can help people who are more interested in this field get started faster. Therefore, the first demand of FCLib is to be as simple as possible and rely less third libraris. Relatively speaking, FCLib has no advantage in efficiency and speed. In example , you can see that using FCLib, you can easily build a simple RGBD SLAM system, and finally extract a three-dimensional model, as shown below: FBAFusion in example Get Started The FCLib development environment is ubuntu 16.04, but it has very few dependent libraries, so there should be no problem to build with ubuntu 18. The two main dependencies are OpenCV and Eigen. Other small dependencies are integrated into 3rdParty. Generally speaking, as long as there are OpenCV and Eigen in the environment, it can be compiled and run with one click. Install OpenCV(3.x): baidu / google Install Eigen\uff083.x\uff09: baidu / google mkdir build && cd build && cmake .. && make In addition, why not design to be less dependent? In addition to the poor level of the author, another point is that OpenCV and Eigen are used widely. If all io interfaces are used at a lower level, it is not convenient for the library to be used in more other projects (the author firstly used Open3D in one project. Because Open3D image reading uses a lower-level library instead of OpenCV, and the project uses OpenCV mat. The format conversion becomes very troublesome. So he decided to implemented the algorithm by hiself. This code became the earliest piece in FCLib). Download The code is released in github: FCLib . Test data can be downloaded from TestData .","title":"Brief Introduction"},{"location":"#welcome-to-fclib","text":"\u70b9\u51fb \u8fd9\u91cc \u67e5\u770b\u4e2d\u6587\u8bf4\u660e\u6587\u6863\u3002 FCLib (fucking cool library) is a C++ library for RGB-D based SLAM and 3D vision. This library is mainly a summary of some basic knowledge and is used to satisfy the author's obsessive-compulsive disorder. It tries to be designed to be relatively simple, clear in structure, easy to understand, and friendly to novices, but limited to the level of the author, there may still be many areas where the design is not good enough. Currently this library does not use GPU except for the visualization part. The author hopes that FCLib can help people who are more interested in this field get started faster. Therefore, the first demand of FCLib is to be as simple as possible and rely less third libraris. Relatively speaking, FCLib has no advantage in efficiency and speed. In example , you can see that using FCLib, you can easily build a simple RGBD SLAM system, and finally extract a three-dimensional model, as shown below: FBAFusion in example","title":"Welcome to FCLib"},{"location":"#get-started","text":"The FCLib development environment is ubuntu 16.04, but it has very few dependent libraries, so there should be no problem to build with ubuntu 18. The two main dependencies are OpenCV and Eigen. Other small dependencies are integrated into 3rdParty. Generally speaking, as long as there are OpenCV and Eigen in the environment, it can be compiled and run with one click. Install OpenCV(3.x): baidu / google Install Eigen\uff083.x\uff09: baidu / google mkdir build && cd build && cmake .. && make In addition, why not design to be less dependent? In addition to the poor level of the author, another point is that OpenCV and Eigen are used widely. If all io interfaces are used at a lower level, it is not convenient for the library to be used in more other projects (the author firstly used Open3D in one project. Because Open3D image reading uses a lower-level library instead of OpenCV, and the project uses OpenCV mat. The format conversion becomes very troublesome. So he decided to implemented the algorithm by hiself. This code became the earliest piece in FCLib).","title":"Get Started"},{"location":"#download","text":"The code is released in github: FCLib . Test data can be downloaded from TestData .","title":"Download"},{"location":"about/","text":"About The Author Guoqing Zhang I received my bachelor degree from Xidian University, now I am a master candidate in TBSI, Tsinghua University. My research interests are 3D vision and computer graphics. email: wlsdzyzl@163.com Github: MyEvolution bilibili: _\u65e0\u804a\u65f6\u7684\u81ea\u5a31\u81ea\u4e50 Blog\uff1a wlsdzyzl (\u505c\u66f4) Space: Evolution In every single place, every school I went, I dreamed of being that cool kid, Even if it meant acting stupid.","title":"About Author"},{"location":"about/#about-the-author","text":"","title":"About The Author"},{"location":"about/#guoqing-zhang","text":"I received my bachelor degree from Xidian University, now I am a master candidate in TBSI, Tsinghua University. My research interests are 3D vision and computer graphics. email: wlsdzyzl@163.com Github: MyEvolution bilibili: _\u65e0\u804a\u65f6\u7684\u81ea\u5a31\u81ea\u4e50 Blog\uff1a wlsdzyzl (\u505c\u66f4) Space: Evolution In every single place, every school I went, I dreamed of being that cool kid, Even if it meant acting stupid.","title":"Guoqing Zhang"},{"location":"changelog/","text":"Change Log 2020.9.17: Add geometry::KDTree (encapsulation of OpenCV); add some supports for Scannet database, such as reading frame sequence, aligning depth camera and color camera, generating model, etc.; Add supporting reading and writing of PLY file for any other vertex properties. 2020.9.21: Delete RPLYManager , add PLYManager . Use tinyply for reading and writing of ply files instead of rply . tinyply is the cpp11 based library, which is more elegant. At the same time, it improves the methods of reading and writing of arbitrary elements and properties as well as comments. 2020.9.23: Change the implementation of KDTree to use nanoflann , which is more efficient in memory and running time than OpenCV's flann; the change log column is added to the documentation.","title":"ChangeLog"},{"location":"changelog/#change-log","text":"2020.9.17: Add geometry::KDTree (encapsulation of OpenCV); add some supports for Scannet database, such as reading frame sequence, aligning depth camera and color camera, generating model, etc.; Add supporting reading and writing of PLY file for any other vertex properties. 2020.9.21: Delete RPLYManager , add PLYManager . Use tinyply for reading and writing of ply files instead of rply . tinyply is the cpp11 based library, which is more elegant. At the same time, it improves the methods of reading and writing of arbitrary elements and properties as well as comments. 2020.9.23: Change the implementation of KDTree to use nanoflann , which is more efficient in memory and running time than OpenCV's flann; the change log column is added to the documentation.","title":"Change Log"},{"location":"examples/","text":"Examples Here are some examples of using FCLib. All the codes are in directory examples . After compilation, you can find the binary files in build/example and enter following commend to test. Read PLY Files and Visualize the Point Cloud or Mesh ./ReadPLYPCD [filename.ply] ./ReadPLYMesh [filename.ply] Read Point Cloud from RGB and Depth ./ReadRGBD [color] [depth] SparseTracking and DenseTracking ./SparseOdometry [source_rgb] [source_depth] [target_rgb] [targe_depth] ./SparseOdometryMild [source_rgb] [source_depth] [target_rgb] [targe_depth] ./DenseOdometry [source_rgb] [source_depth] [target_rgb] [targe_depth] Generate Model from Scannet dataset example/GenerateModelFromScannet.cpp can generate a model based on the given color, depth and camera pose. Note that there is an alignment between color and depth because of different camera intrinsics. ./GenerateModelFromScannet [Path] GetLabelUsingKDTree.cpp can infer the semantic label of each point in generated model based on KDTree and the given labeled model. ./GetLabelUsingKDTree [original model path] [reference model path] Mesh Simplification ./SimplifyMeshClustering [filename.ply] [grid_length] ./SimplifyMeshQuadric [filename.ply] [simplify_ratio] Here are some resuls: original model (hand.ply) using quadrics simplification(simplification rate = 90%) using voxel clustering (voxel_len = 0.1) Bundle Adjustment Input a pair of RGBD frames,the camera poses will be adjusted according to the result of feature matching by Bundle Adjutment, and then the matching result will be visualized. You can also find several images in current directory which plot the reprojection results. ./BATest [source_rgb] [source_depth] [target_rgb] [targe_depth] The result will be like: alignment after BA reprojection error comparison The BA optimize the camera poses and coordinates of 3D points jointly. However, the alignment is just based on camera poses and camera intrinsics and don't utilize the coordinates parts. So generally the comparison of reprojection error should looks better than point cloud alignment. ICP ICPTest [source_pcd] [target_pcd] ICPTest registers two point clouds. You can choose to use PointToPlane or PointToPoint by changing the code. From the experiment we can see PointToPlane have a much faster convergence rate. point_to_point point_to_plane RansacRegistration RansacTest [source_pcd] [target_pcd] RansacTest will register two point cloud. At the beginning, the source point cloud will be rotated randomly and translated, so we can see clearly what GlobalRegistration can do. The following images show the results. You can choose to visualize the alignments or correspondences as you want by changing the example code. ransac registration fpfh correspondence Room Detection You can do some interesting things by uisng module algorithm . In example/RoomDetection.cpp , we can get the 2d map of the room from the room model with semantic labels. We use LineDetection, PlaneDetection( acturally we just need to detect the lines or planes), DCEL, KMedoidsClusteringDynamic. ./RoomDetection ../../TestData/room_with_labels.ply Input model: Results: detected lines/planes generated Line Arrangements/2d map For details, refer to Automatic room detection and reconstruction in cluttered indoor environments with complex room layouts . Three Reconstruction Systems A slam system wihout an backend is incomplete. If you just track frames and don't optimize the poses, the results will have ghosting, just like this: FCLib implements three complete SLAM systems, which are different in backend or tracking method. FBAFusion FBAFusion using SparseTracking, Mild loop closure detector,Fast BundleAdjustment for optimization. ./FBAFusion ../../TestData/rgbd/607 0.01 You should see: FBAFusion is the fastest of the three, the generated model looks like: BAFusion BAFusion uses SparseTracking, Mild closed-loop detection, plus BundleAdjustment back-end optimization. As mentioned before, BA will have a larger matrix dimension in later stage. Even in BAFusion, each keyframe has only 30 feature points for coordinate optimization, and it still takes about a minute or more to optimize them. And BA will optimize the spatial points (optimize the projection noise and parameters), but this part of the optimization is not considered when the final model is generated, only the optimized pose is used, so the reconstruction model does not look as good as FBAFusion. However, you can still see the optimization effects (no ghosting). If all aspects are taken into consideration, the effects of BA should be the best. ./BAFusion ../../TestData/rgbd/607 0.01 Generated model: DenseFusion DenseFusion using DenseTracking, without a 2D loop closure detetion. It takes every \\(N\\) of the read frame as a submap. When a new submap is detected, BAFusion perform GlobalRegistration for the current submap and the previous submaps, and then sets a threshold to detect whether there is a correct loop closure. Of course, DenseTracking and GlobalRegistration are much slower than SparseTracking and MILD loop closure detection. Therefore, DenseFusion is also slower that FBAFusion and is an offline reconstruction system. As for backend optimization, DenseFusion uses Fast BundleAdjustment. ./DenseFusion ../../TestData/rgbd/607 0.01 Generated model: Note: The color looks strange because of the default BGR format of OpenCV.","title":"Examples"},{"location":"examples/#examples","text":"Here are some examples of using FCLib. All the codes are in directory examples . After compilation, you can find the binary files in build/example and enter following commend to test.","title":"Examples"},{"location":"examples/#read-ply-files-and-visualize-the-point-cloud-or-mesh","text":"./ReadPLYPCD [filename.ply] ./ReadPLYMesh [filename.ply]","title":"Read PLY Files and Visualize the Point Cloud or Mesh"},{"location":"examples/#read-point-cloud-from-rgb-and-depth","text":"./ReadRGBD [color] [depth]","title":"Read Point Cloud from RGB and Depth"},{"location":"examples/#sparsetracking-and-densetracking","text":"./SparseOdometry [source_rgb] [source_depth] [target_rgb] [targe_depth] ./SparseOdometryMild [source_rgb] [source_depth] [target_rgb] [targe_depth] ./DenseOdometry [source_rgb] [source_depth] [target_rgb] [targe_depth]","title":"SparseTracking and DenseTracking"},{"location":"examples/#generate-model-from-scannet-dataset","text":"example/GenerateModelFromScannet.cpp can generate a model based on the given color, depth and camera pose. Note that there is an alignment between color and depth because of different camera intrinsics. ./GenerateModelFromScannet [Path] GetLabelUsingKDTree.cpp can infer the semantic label of each point in generated model based on KDTree and the given labeled model. ./GetLabelUsingKDTree [original model path] [reference model path]","title":"Generate Model from Scannet dataset"},{"location":"examples/#mesh-simplification","text":"./SimplifyMeshClustering [filename.ply] [grid_length] ./SimplifyMeshQuadric [filename.ply] [simplify_ratio] Here are some resuls: original model (hand.ply) using quadrics simplification(simplification rate = 90%) using voxel clustering (voxel_len = 0.1)","title":"Mesh Simplification"},{"location":"examples/#bundle-adjustment","text":"Input a pair of RGBD frames,the camera poses will be adjusted according to the result of feature matching by Bundle Adjutment, and then the matching result will be visualized. You can also find several images in current directory which plot the reprojection results. ./BATest [source_rgb] [source_depth] [target_rgb] [targe_depth] The result will be like: alignment after BA reprojection error comparison The BA optimize the camera poses and coordinates of 3D points jointly. However, the alignment is just based on camera poses and camera intrinsics and don't utilize the coordinates parts. So generally the comparison of reprojection error should looks better than point cloud alignment.","title":"Bundle Adjustment"},{"location":"examples/#icp","text":"ICPTest [source_pcd] [target_pcd] ICPTest registers two point clouds. You can choose to use PointToPlane or PointToPoint by changing the code. From the experiment we can see PointToPlane have a much faster convergence rate. point_to_point point_to_plane","title":"ICP"},{"location":"examples/#ransacregistration","text":"RansacTest [source_pcd] [target_pcd] RansacTest will register two point cloud. At the beginning, the source point cloud will be rotated randomly and translated, so we can see clearly what GlobalRegistration can do. The following images show the results. You can choose to visualize the alignments or correspondences as you want by changing the example code. ransac registration fpfh correspondence","title":"RansacRegistration"},{"location":"examples/#room-detection","text":"You can do some interesting things by uisng module algorithm . In example/RoomDetection.cpp , we can get the 2d map of the room from the room model with semantic labels. We use LineDetection, PlaneDetection( acturally we just need to detect the lines or planes), DCEL, KMedoidsClusteringDynamic. ./RoomDetection ../../TestData/room_with_labels.ply Input model: Results: detected lines/planes generated Line Arrangements/2d map For details, refer to Automatic room detection and reconstruction in cluttered indoor environments with complex room layouts .","title":"Room Detection"},{"location":"examples/#three-reconstruction-systems","text":"A slam system wihout an backend is incomplete. If you just track frames and don't optimize the poses, the results will have ghosting, just like this: FCLib implements three complete SLAM systems, which are different in backend or tracking method.","title":"Three Reconstruction Systems"},{"location":"examples/#fbafusion","text":"FBAFusion using SparseTracking, Mild loop closure detector,Fast BundleAdjustment for optimization. ./FBAFusion ../../TestData/rgbd/607 0.01 You should see: FBAFusion is the fastest of the three, the generated model looks like:","title":"FBAFusion"},{"location":"examples/#bafusion","text":"BAFusion uses SparseTracking, Mild closed-loop detection, plus BundleAdjustment back-end optimization. As mentioned before, BA will have a larger matrix dimension in later stage. Even in BAFusion, each keyframe has only 30 feature points for coordinate optimization, and it still takes about a minute or more to optimize them. And BA will optimize the spatial points (optimize the projection noise and parameters), but this part of the optimization is not considered when the final model is generated, only the optimized pose is used, so the reconstruction model does not look as good as FBAFusion. However, you can still see the optimization effects (no ghosting). If all aspects are taken into consideration, the effects of BA should be the best. ./BAFusion ../../TestData/rgbd/607 0.01 Generated model:","title":"BAFusion"},{"location":"examples/#densefusion","text":"DenseFusion using DenseTracking, without a 2D loop closure detetion. It takes every \\(N\\) of the read frame as a submap. When a new submap is detected, BAFusion perform GlobalRegistration for the current submap and the previous submaps, and then sets a threshold to detect whether there is a correct loop closure. Of course, DenseTracking and GlobalRegistration are much slower than SparseTracking and MILD loop closure detection. Therefore, DenseFusion is also slower that FBAFusion and is an offline reconstruction system. As for backend optimization, DenseFusion uses Fast BundleAdjustment. ./DenseFusion ../../TestData/rgbd/607 0.01 Generated model: Note: The color looks strange because of the default BGR format of OpenCV.","title":"DenseFusion"},{"location":"rules/","text":"Some Rules Since FCLib is expected to be friendly to beginners, it is necessary to make the code structure as clear as possible. Here are some rules that FCLib followed when it was written. First of all, except for examples, all classes, functions, etc., are included in the fucking_cool namespace. FCLib is divided into several modules, and each module has its own namespace, which are camera ( camera ), geometry ( geometry ), visual odometry ( odometry ), loop closure detection ( lcdetection ), point cloud Registration ( registration ), optimization ( optimization ), generative model ( integration ), visualization ( visualization ), algorithm ( algorithm ) and other useful tools ( tool ). Directory and file names are all composed of uppercase letters and lowercase letters ( FuckingCool ), names of namespaces are composed of lowercase letters and underline ( fucking_cool ), function and class names are all composed of uppercase and lowercase letters ( FuckingCool ), and the name of a specific object is composed of lowercase and underlined letters ( fucking_cool ) (basically this is the case, some functions may break the rules due to the negligence). For some special values \u200b\u200bor parameters defined in advance, use all uppercase and undersline ( MAX_VALUE ). Code indentation refers to python syntax. The author writes poorly comments. But related papers will be given in the related_paper folder. In examples , the author gives several examples, which will also be introduced in the following content. Just ignore the codes in examples that are not introduced.","title":"Some Rules"},{"location":"rules/#some-rules","text":"Since FCLib is expected to be friendly to beginners, it is necessary to make the code structure as clear as possible. Here are some rules that FCLib followed when it was written. First of all, except for examples, all classes, functions, etc., are included in the fucking_cool namespace. FCLib is divided into several modules, and each module has its own namespace, which are camera ( camera ), geometry ( geometry ), visual odometry ( odometry ), loop closure detection ( lcdetection ), point cloud Registration ( registration ), optimization ( optimization ), generative model ( integration ), visualization ( visualization ), algorithm ( algorithm ) and other useful tools ( tool ). Directory and file names are all composed of uppercase letters and lowercase letters ( FuckingCool ), names of namespaces are composed of lowercase letters and underline ( fucking_cool ), function and class names are all composed of uppercase and lowercase letters ( FuckingCool ), and the name of a specific object is composed of lowercase and underlined letters ( fucking_cool ) (basically this is the case, some functions may break the rules due to the negligence). For some special values \u200b\u200bor parameters defined in advance, use all uppercase and undersline ( MAX_VALUE ). Code indentation refers to python syntax. The author writes poorly comments. But related papers will be given in the related_paper folder. In examples , the author gives several examples, which will also be introduced in the following content. Just ignore the codes in examples that are not introduced.","title":"Some Rules"},{"location":"thanksto/","text":"Acknowledgments Thanks to Open3D , Google , SCIHub , Github , arxiv , Luvision Lab and members . Special thanks to Lei Han .","title":"Acknowledgments"},{"location":"thanksto/#acknowledgments","text":"Thanks to Open3D , Google , SCIHub , Github , arxiv , Luvision Lab and members . Special thanks to Lei Han .","title":"Acknowledgments"},{"location":"modules/algorithm/","text":"Some Algorithms namespace: algorithm FCLib also implements some other algorithms. This part of the content is a summary of algorithms used in author's projects. As you will see in example , using this part can also do some interesting work. Clutering.h Three clustering algorithms are implemented in Clutering.h : KMeans(based on OpenCV), MeansShift, KMediods. //KMeans based on opencv template <int T > void KMeansClustering(const std::vector<Eigen::Matrix<geometry::scalar,T,1>, Eigen::aligned_allocator<Eigen::Matrix<geometry::scalar,T,1>> > & wait_to_cluster, std::vector<Cluster<T>> &clustering_result, int K); //MeansShift template <int T > void MeansShiftClustering(const std::vector<Eigen::Matrix<geometry::scalar,T,1>, Eigen::aligned_allocator<Eigen::Matrix<geometry::scalar,T,1>> > & wait_to_cluster, std::vector<Cluster<T>> &clustering_result, float radius); //KMedoids template <int T > void KMedoidsClustering(const std::vector<Eigen::Matrix<geometry::scalar,T,1>, Eigen::aligned_allocator<Eigen::Matrix<geometry::scalar,T,1>> > & wait_to_cluster, std::vector<Cluster<T>> &clustering_result, int target_number, bool initialized = false, const std::vector<int> & initialized_index = std::vector<int>()); //For the above three kinds of clustering, you need to determine the vector dimension of the clustered object when writing the code //KMedoidsClusteringDynamic can handle the dynamic vector dimension void KMedoidsClusteringDynamic(const geometry::PointXList & wait_to_cluster, std::vector<ClusterDynamic> &clustering_result, int target_number, bool initialized = false, const std::vector<int> & initialized_index = std::vector<int>()); PatchDetection.h Line detection for 2D points and plane detection for 3D points are implemented in PatchDetection.h . void LineDetection(const geometry::Point2List &points, std::vector<LinePatch> &results); void PlaneDetection(const geometry::Point3List &points, std::vector<PlanePatch> &Patches); DCEL.h DCEL.h implemented a 'Doubly Connected Edge List', a famous data structure in computational geometry, which can be used to depict the 'Line Arrangements'('Cell Complex'). After many lines(planes) intersect in a space, some grids are formed. These grids are like cells, which are so called cell complex.If you use DCEL to describe it, you can add lines to it at will (or remove a certain line, which is rarely used). It is also easy to find the grids through which a certain line passes, and to locate the grid that contains a given point, with little computation. Anyway, DCEL is a very powerful tool. Line Arrangements DCEL has two main member function: //Insert a line into DCEL void IncrementLine(const geometry::Line &line); //Remove a certain line(Needs more tests) void ReductLine(int line_id); Although it looks quiet easy, the implementation is actually a little complex. Arrangements.h Arrangements.h are used to help build a 'DCEL'. It can be used to find all the intersection points of multiple lines to determine the volume of the bounding box. You can see a demostration of Line Arrangements here: Line Arrangements Using Algorithm , we can implement a 2D map detection of rooms .","title":"Some Algorithms"},{"location":"modules/algorithm/#some-algorithms","text":"namespace: algorithm FCLib also implements some other algorithms. This part of the content is a summary of algorithms used in author's projects. As you will see in example , using this part can also do some interesting work.","title":"Some Algorithms"},{"location":"modules/algorithm/#cluteringh","text":"Three clustering algorithms are implemented in Clutering.h : KMeans(based on OpenCV), MeansShift, KMediods. //KMeans based on opencv template <int T > void KMeansClustering(const std::vector<Eigen::Matrix<geometry::scalar,T,1>, Eigen::aligned_allocator<Eigen::Matrix<geometry::scalar,T,1>> > & wait_to_cluster, std::vector<Cluster<T>> &clustering_result, int K); //MeansShift template <int T > void MeansShiftClustering(const std::vector<Eigen::Matrix<geometry::scalar,T,1>, Eigen::aligned_allocator<Eigen::Matrix<geometry::scalar,T,1>> > & wait_to_cluster, std::vector<Cluster<T>> &clustering_result, float radius); //KMedoids template <int T > void KMedoidsClustering(const std::vector<Eigen::Matrix<geometry::scalar,T,1>, Eigen::aligned_allocator<Eigen::Matrix<geometry::scalar,T,1>> > & wait_to_cluster, std::vector<Cluster<T>> &clustering_result, int target_number, bool initialized = false, const std::vector<int> & initialized_index = std::vector<int>()); //For the above three kinds of clustering, you need to determine the vector dimension of the clustered object when writing the code //KMedoidsClusteringDynamic can handle the dynamic vector dimension void KMedoidsClusteringDynamic(const geometry::PointXList & wait_to_cluster, std::vector<ClusterDynamic> &clustering_result, int target_number, bool initialized = false, const std::vector<int> & initialized_index = std::vector<int>());","title":"Clutering.h"},{"location":"modules/algorithm/#patchdetectionh","text":"Line detection for 2D points and plane detection for 3D points are implemented in PatchDetection.h . void LineDetection(const geometry::Point2List &points, std::vector<LinePatch> &results); void PlaneDetection(const geometry::Point3List &points, std::vector<PlanePatch> &Patches);","title":"PatchDetection.h"},{"location":"modules/algorithm/#dcelh","text":"DCEL.h implemented a 'Doubly Connected Edge List', a famous data structure in computational geometry, which can be used to depict the 'Line Arrangements'('Cell Complex'). After many lines(planes) intersect in a space, some grids are formed. These grids are like cells, which are so called cell complex.If you use DCEL to describe it, you can add lines to it at will (or remove a certain line, which is rarely used). It is also easy to find the grids through which a certain line passes, and to locate the grid that contains a given point, with little computation. Anyway, DCEL is a very powerful tool. Line Arrangements DCEL has two main member function: //Insert a line into DCEL void IncrementLine(const geometry::Line &line); //Remove a certain line(Needs more tests) void ReductLine(int line_id); Although it looks quiet easy, the implementation is actually a little complex.","title":"DCEL.h"},{"location":"modules/algorithm/#arrangementsh","text":"Arrangements.h are used to help build a 'DCEL'. It can be used to find all the intersection points of multiple lines to determine the volume of the bounding box. You can see a demostration of Line Arrangements here: Line Arrangements Using Algorithm , we can implement a 2D map detection of rooms .","title":"Arrangements.h"},{"location":"modules/camera/","text":"Pinhole Camera namespace: camera The camera module defines a simple pinhole camera model, includes the camera's intrinsics, distortion parameters (FCLib actually does not use this part, so all default to 0), and the depth range. You can set the camera parameters by your calibration results, or use the camera parameters of the TUM dataset or the general ASUS xtion camera defined in the library . The camera module is very simple. It only contains a header file. You can easily get the camera parameter matrix through ToCameraMatrix to facilitate calculations. As long as you understand the pinhole camera model, you can easily understand this part of the code. A fisheye camera model may be added later.","title":"Camera"},{"location":"modules/camera/#pinhole-camera","text":"namespace: camera The camera module defines a simple pinhole camera model, includes the camera's intrinsics, distortion parameters (FCLib actually does not use this part, so all default to 0), and the depth range. You can set the camera parameters by your calibration results, or use the camera parameters of the TUM dataset or the general ASUS xtion camera defined in the library . The camera module is very simple. It only contains a header file. You can easily get the camera parameter matrix through ToCameraMatrix to facilitate calculations. As long as you understand the pinhole camera model, you can easily understand this part of the code. A fisheye camera model may be added later.","title":"Pinhole Camera"},{"location":"modules/geometry/","text":"Geometry namespace: geometry The geometry module is a foundation of the entire system. It defines some very basic data structures and implements basic operations and algorithms, including points, transformation matrix, the conversion between Lie groups and Lie algebras (SE(3), se(3)) etc., as well as the definition and implementation of common data structures such as point clouds, triangular mesh and related algorithms. Geometry.h In Geometry.h , the first thing you shuold notice is that: typedef float scalar; By default, float is used as the scalar type, so the format of point clouds, matrix, etc. are all single-precision, and the floating-point number precision used by the entire library can be changed here. typedef Vector6 Se3; typedef Matrix4 SE3; Lie group Lie algebras are defined as SE3 and Se3 , which are four-dimensional square matrix and six-dimensional matrix, respectively, through the function: Matrix4 Se3ToSE3(const Vector6 &input); Vector6 SE3ToSe3(const Matrix4 &input); you can do the conversion. These functions are implemented based on Sophus library(In Open3D, the mutual conversion from a 6-dimensional vector to a 4-dimensional square matrix is completely unchanged for the translation part, but from the mathematical derivation, there is a slight difference between the two, so the Sophus library should be more accurate). struct VoxelGridHasher; struct PixelGridHasher; The above two are hash functions for three-dimensional and two-dimensional integer vectors, and spatial hashing is a very efficient data structure. std::tuple<Point3, double, double> FitPlane(const Point3List & _points); std::tuple<Vector2, double, double> FitLine(const Point2List & _points); The above functions are used to fit a set of 3D point planes and 2D point lines. For example, in a three-dimensional coordinate system, the parametric equation of a plane is: \\(ax+by+cz+d = 0\\) , where \\(n=\\{ a, b, c\\)^T\\) is the normal vector of the plane. The return value type of the above function is std::tuple , where the first term is the normal vector, and the second is \\(d\\) . The third one is a ratio of the second largest singular value to the largest singular value (using SVD to fit a plane or a straight line, you can get singular values), which can be used as an indicator of the error between the original points and the fitted plane. If all points are in On the plane, the value is 0. That is, the better the plane fits, the smaller the value. FCLib use Point3List to denote an array of Eigen::Matrix<geometry::scalar, 3, 1> , similarly, PointXList represents an array of Eigen::Matrix<scalar, Eigen::Dynamic, 1> . Also, with cpp 11, we can give an alias for Eigen::Matrix<geometry::Scalar, T, 1> by: template <int T> using Vector = Eigen::Matrix<scalar, T, 1>; So you can use Vector<10> to declare a 10D vector. Therefore, we also use PointList to denote an array of template vector, which means we use PointList<100> as an alias of std::vector<Eigen::Matrix<scalar, 100, 1>, Eigen::aligned_allocator<Eigen::Matrix<scalar, 100, 1>>> . This is much useful. In Geometry.h , some other aliases are also defined. These aliases are defined for easy-understanding. Some basic knowledges of Feature Matching, etc. is needed to understand the meanings. Geometry2d.h Geometry2d.h implements some geometric algorithms on 2d, mainly to calculate some very basic content of geometry. This part is written for DCEL (doubly connected edge list) in Algorithm . The main functions in this part are: //Check if a point is inside a convex polygon int CheckPointInConvexPoly(const geometry::Point2List &points, const Point2 &p); //Calculate the area of a convex polygon float ComputeAreaConvexPoly(const Point2List &points); PointCloud.h PointCloud.h implements some point cloud-related data structures and algorithms. The point cloud contains points, colors, and normals (the latter two are optional). The related algorithms include point cloud downsampling, normal vector estimation, and so on. Some basic member functions are listed below: //Read PCD from PLY file void LoadFromPLY(const std::string &filename); //Read PCD from OBJ file void LoadFromOBJ(const std::string &filename); //Read PCD from color and depth void LoadFromRGBD(const cv::Mat &rgb, const cv::Mat & depth, const camera::PinholeCamera &camera ); void LoadFromRGBD(const RGBDFrame &rbgd, const camera::PinholeCamera &camera ); //Estimate the normal vector by fitting a plane to the surrounding points of a certain point. //`radius` represents the search radius, and `knn` represents the number of fitting points void EstimateNormals(float radius = 0.1, int knn = 30); //Transform pcd void Transform(const TransformationMatrix &T); //Down sample std::shared_ptr<PointCloud> DownSample(double grid_len); //Write pcd to PLY file bool WriteToPLY(const std::string &fileName) const; //Write pcd to OBJ file bool WriteToOBJ(const std::string &fileName) const; //Merge this pcd with another one void MergePCD(const PointCloud & another_pcd); TriangleMesh.h TriangleMesh defines the triangular mesh data structures and related algorithms. The biggest difference between a triangular mesh and a point cloud is that there are edges and faces in triangle mesh. In the triangular mesh, each face is a triangle and consists of 3 points. Therefore, there is also a face table in the triangular mesh, which stores the indexes of the 3 points of each triangle. The currently implemented grid does not contain textures. //Read mesh from PLY file void LoadFromPLY(const std::string &filename); // Read Mesh from OBJ file void LoadFromOBJ(const std::string &filename); /* Calculating the normal vector, the normal vector of the triangular grid is easier to estimate compared to the normal vector of the pcd. First of all, the triangular mesh already has a face, and you can directly find the plane based on the three points to get the normal vector of the face. Then, we only need to perform a weighted average of the normal vectors of all the faces connected to the point to get the normal vector of a point. */ void ComputeNormals(float radius = 0.1, int knn = 30); //Transform the mesh void Transform(const TransformationMatrix &T); //Write mesh to PLY file bool WriteToPLY(const std::string &fileName) const; //Write mesh to OBJ file bool WriteToOBJ(const std::string &fileName) const; //Simplify mesh with edge collapse based on quadric error std::shared_ptr<geometry::TriangleMesh> QuadricSimplify(int target_num) const; //Voxel-based uniform grid simplification, similar to point cloud downsampling std::shared_ptr<geometry::TriangleMesh> ClusteringSimplify(float grid_len) const; //Remove fragments whose connected points is less than a threshold std::shared_ptr<geometry::TriangleMesh> Prune(int min_points) const; For details of quadric simplification, refer to: related_paper/quadrics.pdf . Comparison of simplification: Simplification Ransac.h Ransac.h mainly implements two specific algorithms that use ransac, based on a basic Ransac framework GRANSAC . The advantage of Ransac is that it will randomly extract some points are used to find a certain transformation or model, so that outliers can be largely excluded. For example, when fitting a plane, if you use the FitPlane in Geometry.h to directly fit, then the outlier will have a great impact on the result. And using Ransac can get better results. //Use Ransac to estimate the relative transformation TransformationMatrix EstimateRigidTransformationRANSAC(const PointCorrespondenceSet &correspondence_set, PointCorrespondenceSet & inliers, std::vector<int> &inlier_ids, int max_iteration = 2000, float threshold = 0.1); //Use Ransac to fit plane std::tuple<geometry::Point3, double, double> FitPlaneRANSAC(const Point3List &points, Point3List & inliers, std::vector<int> &inlier_ids, int max_iteration = 2000, float threshold = 0.05); You can get the inlies and corresponding indices using above function. RGBDFrame.h RGBDFrame.h defines the RGBDFrame data structure, which integrates the depth map and the color map, and can simplify the calling process. This data structure is mainly used to serve an RGBD SLAM system. It can save some of the extracted features, point cloud and so on, which are needed for tracking. When building an RGBD SLAM, for efficiency these contents do not need to be recalculated. Release() is used to release all saved content. KDTree.h There are two underlying implementations of KDTree, which are based on OpenCV and nanoflann respectively. You can choose any of the by settingt the pre-defined variable NANO_IMPLAMENTATION . OpenCV (Not Recommended) The most troublesome part of calling OpenCV's KDTree is the mutual conversion between Eigen and OpenCv, and in fact, geometry::KDTree is to solve this. The following is a piece of code (from the Internet) that uses kdtree in OpenCV. int main() { //Points to build tree vector<cv::Point2f> features = { { 1,1 },{ 2, 2},{ 3, 3},{ 4, 4},{ 2, 4} }; cv::Mat source = cv::Mat(features).reshape(1); source.convertTo(source, CV_32F); cv::flann::KDTreeIndexParams indexParams(2); cv::flann::Index kdtree(source, indexParams); //Parameter int queryNum = 3;//knn vector<float> vecQuery(2);//query point vector<int> vecIndex(queryNum);//indices of the k nearest points vector<float> vecDist(queryNum);//distances cv::flann::SearchParams params(32);//max search depth vecQuery = { 3, 4}; kdtree.knnSearch(vecQuery, vecIndex, vecDist, queryNum, params); cout << \"vecDist: \" << endl; for (auto&x : vecDist) cout << x << \" \"; cout << endl; cout << \"vecIndex: \" << endl; for (auto&x : vecIndex) cout << x << \" \"; return 0; } There is some points you should be careful: 1. FLANN in OpenCV does not support double 2. In this part, the type of query point is vector<float> \uff0cif you are using cv::Mat(features[0]) , there will be a segmentation fault. 3. You need to maintain the input array all the time. So if you want to separate the building process and searching process, you need to maintain a shared matrix cloned from the origin matrix. FCLib uses a member( input_array ) in class. 4. knnSearch won't return any value, and you need to do a resize(k) for indices and distances array before searching. In radiusSearch , you don't have to resize the indices and distances array. radiusSearch will reture a number of searched points. You need to use the retured value to iterate the indices and distance array rather than indices.size() , because indices.size() is equal to max_points you set before, which is the maximum number of points you want to search. Anyway, you have no needs to worry about these problems in FCLib. In FCLib , KDTree is very simple to use. It is a template class, that is, you need to specify the dimensions of the data placed in the tree. When building the tree, directly input the PointList series defined in geometry.h (essentially the vector of the Eigen vector), which can be a dynamic vector array or a static vector array. If it is a dynamic vector array, the dimension of the dynamic vector needs to be equal to the dimension of the template. In addition, searching through RadiusSearch and KnnSearch can also be directly based on the vector of Eigen , and FCLib doesn't have any assumption about the indices array and distances array. nanoflann nanoflann is a very lightweight C++11 library, with only one hpp file. Compared with the implementation of OpenCV, nanoflann is more efficient and occupies less memory. In nanoflann, you need to declare a class for each kinds of points(such as 2D or 3D points) that needs to be searched, which contains certain functions. In FCLib, there is no need to do this. Nanoflann's radiusSearch cannot specify max_neighbors , which means it will find all points within the radius. This is not necessary in some cases and can cause waste of computing resources. FCLib modified the source code of nanoflann to get this feature (all points will be searched when max_neighbors =0). The KDTree based on nanoflann implementation has the same usage of KDTree based on OpenCV. Additionally, it supports indices of std::vector<size_t> . class SearchParameter { public: //kdtree parameter SearchParameter(int _checks = 256, float _eps = 1e-8, bool _sorted = true) { checks = _checks; eps = _eps; sorted = _sorted; } //the indices array is sorted based on the distances bool sorted = true; //EPS float eps = 1e-8; //Max search depth int checks = 256; }; template<int T = 3> class KDTree { public: KDTree(int _trees = 4) { trees = 4; } //To build KDTree from a set of high-dimensional vectors, you can choose to use a dynamic vector vector or a static vector vector void BuildTree(const geometry::PointXList &points); void BuildTree(const geometry::PointList<T> &points); //strange rules in opencv flann: //when you do knnsearch, you need to resize(k) at beginning. //when you do radius search, you need to get the returned points_count and do a resize for the results. //search points within radius for a dynamic vector or static vector void RadiusSearch(const geometry::VectorX &point, std::vector<int> &indices, std::vector<float > &dists, double radius, int max_result, const SearchParameter &sp = SearchParameter()); void RadiusSearch(const geometry::Vector<T> &point, std::vector<int> &indices, std::vector<float > &dists, double radius, int max_result, const SearchParameter sp = SearchParameter()); //search the k nearest points for a dynamic vector or static vector void KnnSearch(const geometry::VectorX &point, std::vector<int> &indices, std::vector<float > &dists, int k, const SearchParameter &sp = SearchParameter()); void KnnSearch(const geometry::Vector<T> &point, std::vector<int> &indices, std::vector<float > &dists, int k, const SearchParameter &sp = SearchParameter()); #if NANO_IMPLEMENTATION //only nanoflann based KDTree supports void RadiusSearch(const geometry::VectorX &point, std::vector<size_t> &indices, std::vector<float > &dists, double radius, int max_result, const SearchParameter &sp = SearchParameter()); void RadiusSearch(const geometry::Vector<T> &point, std::vector<size_t> &indices, std::vector<float > &dists, double radius, int max_result, const SearchParameter sp = SearchParameter()); //search the k nearest points for a dynamic vector or static vector void KnnSearch(const geometry::VectorX &point, std::vector<size_t> &indices, std::vector<float > &dists, int k, const SearchParameter &sp = SearchParameter()); void KnnSearch(const geometry::Vector<T> &point, std::vector<size_t> &indices, std::vector<float > &dists, int k, const SearchParameter &sp = SearchParameter()); #endif }; You can view example/GetLabelUsingKDTree.cpp to better understand how to use KDTree in FCLib. The input is two models: original model (without semantic information) and reference model (with semantic information). By finding the closest point through KDTree, we can obtain the original model's semantic labels.","title":"Geometry"},{"location":"modules/geometry/#geometry","text":"namespace: geometry The geometry module is a foundation of the entire system. It defines some very basic data structures and implements basic operations and algorithms, including points, transformation matrix, the conversion between Lie groups and Lie algebras (SE(3), se(3)) etc., as well as the definition and implementation of common data structures such as point clouds, triangular mesh and related algorithms.","title":"Geometry"},{"location":"modules/geometry/#geometryh","text":"In Geometry.h , the first thing you shuold notice is that: typedef float scalar; By default, float is used as the scalar type, so the format of point clouds, matrix, etc. are all single-precision, and the floating-point number precision used by the entire library can be changed here. typedef Vector6 Se3; typedef Matrix4 SE3; Lie group Lie algebras are defined as SE3 and Se3 , which are four-dimensional square matrix and six-dimensional matrix, respectively, through the function: Matrix4 Se3ToSE3(const Vector6 &input); Vector6 SE3ToSe3(const Matrix4 &input); you can do the conversion. These functions are implemented based on Sophus library(In Open3D, the mutual conversion from a 6-dimensional vector to a 4-dimensional square matrix is completely unchanged for the translation part, but from the mathematical derivation, there is a slight difference between the two, so the Sophus library should be more accurate). struct VoxelGridHasher; struct PixelGridHasher; The above two are hash functions for three-dimensional and two-dimensional integer vectors, and spatial hashing is a very efficient data structure. std::tuple<Point3, double, double> FitPlane(const Point3List & _points); std::tuple<Vector2, double, double> FitLine(const Point2List & _points); The above functions are used to fit a set of 3D point planes and 2D point lines. For example, in a three-dimensional coordinate system, the parametric equation of a plane is: \\(ax+by+cz+d = 0\\) , where \\(n=\\{ a, b, c\\)^T\\) is the normal vector of the plane. The return value type of the above function is std::tuple , where the first term is the normal vector, and the second is \\(d\\) . The third one is a ratio of the second largest singular value to the largest singular value (using SVD to fit a plane or a straight line, you can get singular values), which can be used as an indicator of the error between the original points and the fitted plane. If all points are in On the plane, the value is 0. That is, the better the plane fits, the smaller the value. FCLib use Point3List to denote an array of Eigen::Matrix<geometry::scalar, 3, 1> , similarly, PointXList represents an array of Eigen::Matrix<scalar, Eigen::Dynamic, 1> . Also, with cpp 11, we can give an alias for Eigen::Matrix<geometry::Scalar, T, 1> by: template <int T> using Vector = Eigen::Matrix<scalar, T, 1>; So you can use Vector<10> to declare a 10D vector. Therefore, we also use PointList to denote an array of template vector, which means we use PointList<100> as an alias of std::vector<Eigen::Matrix<scalar, 100, 1>, Eigen::aligned_allocator<Eigen::Matrix<scalar, 100, 1>>> . This is much useful. In Geometry.h , some other aliases are also defined. These aliases are defined for easy-understanding. Some basic knowledges of Feature Matching, etc. is needed to understand the meanings.","title":"Geometry.h"},{"location":"modules/geometry/#geometry2dh","text":"Geometry2d.h implements some geometric algorithms on 2d, mainly to calculate some very basic content of geometry. This part is written for DCEL (doubly connected edge list) in Algorithm . The main functions in this part are: //Check if a point is inside a convex polygon int CheckPointInConvexPoly(const geometry::Point2List &points, const Point2 &p); //Calculate the area of a convex polygon float ComputeAreaConvexPoly(const Point2List &points);","title":"Geometry2d.h"},{"location":"modules/geometry/#pointcloudh","text":"PointCloud.h implements some point cloud-related data structures and algorithms. The point cloud contains points, colors, and normals (the latter two are optional). The related algorithms include point cloud downsampling, normal vector estimation, and so on. Some basic member functions are listed below: //Read PCD from PLY file void LoadFromPLY(const std::string &filename); //Read PCD from OBJ file void LoadFromOBJ(const std::string &filename); //Read PCD from color and depth void LoadFromRGBD(const cv::Mat &rgb, const cv::Mat & depth, const camera::PinholeCamera &camera ); void LoadFromRGBD(const RGBDFrame &rbgd, const camera::PinholeCamera &camera ); //Estimate the normal vector by fitting a plane to the surrounding points of a certain point. //`radius` represents the search radius, and `knn` represents the number of fitting points void EstimateNormals(float radius = 0.1, int knn = 30); //Transform pcd void Transform(const TransformationMatrix &T); //Down sample std::shared_ptr<PointCloud> DownSample(double grid_len); //Write pcd to PLY file bool WriteToPLY(const std::string &fileName) const; //Write pcd to OBJ file bool WriteToOBJ(const std::string &fileName) const; //Merge this pcd with another one void MergePCD(const PointCloud & another_pcd);","title":"PointCloud.h"},{"location":"modules/geometry/#trianglemeshh","text":"TriangleMesh defines the triangular mesh data structures and related algorithms. The biggest difference between a triangular mesh and a point cloud is that there are edges and faces in triangle mesh. In the triangular mesh, each face is a triangle and consists of 3 points. Therefore, there is also a face table in the triangular mesh, which stores the indexes of the 3 points of each triangle. The currently implemented grid does not contain textures. //Read mesh from PLY file void LoadFromPLY(const std::string &filename); // Read Mesh from OBJ file void LoadFromOBJ(const std::string &filename); /* Calculating the normal vector, the normal vector of the triangular grid is easier to estimate compared to the normal vector of the pcd. First of all, the triangular mesh already has a face, and you can directly find the plane based on the three points to get the normal vector of the face. Then, we only need to perform a weighted average of the normal vectors of all the faces connected to the point to get the normal vector of a point. */ void ComputeNormals(float radius = 0.1, int knn = 30); //Transform the mesh void Transform(const TransformationMatrix &T); //Write mesh to PLY file bool WriteToPLY(const std::string &fileName) const; //Write mesh to OBJ file bool WriteToOBJ(const std::string &fileName) const; //Simplify mesh with edge collapse based on quadric error std::shared_ptr<geometry::TriangleMesh> QuadricSimplify(int target_num) const; //Voxel-based uniform grid simplification, similar to point cloud downsampling std::shared_ptr<geometry::TriangleMesh> ClusteringSimplify(float grid_len) const; //Remove fragments whose connected points is less than a threshold std::shared_ptr<geometry::TriangleMesh> Prune(int min_points) const; For details of quadric simplification, refer to: related_paper/quadrics.pdf . Comparison of simplification: Simplification","title":"TriangleMesh.h"},{"location":"modules/geometry/#ransach","text":"Ransac.h mainly implements two specific algorithms that use ransac, based on a basic Ransac framework GRANSAC . The advantage of Ransac is that it will randomly extract some points are used to find a certain transformation or model, so that outliers can be largely excluded. For example, when fitting a plane, if you use the FitPlane in Geometry.h to directly fit, then the outlier will have a great impact on the result. And using Ransac can get better results. //Use Ransac to estimate the relative transformation TransformationMatrix EstimateRigidTransformationRANSAC(const PointCorrespondenceSet &correspondence_set, PointCorrespondenceSet & inliers, std::vector<int> &inlier_ids, int max_iteration = 2000, float threshold = 0.1); //Use Ransac to fit plane std::tuple<geometry::Point3, double, double> FitPlaneRANSAC(const Point3List &points, Point3List & inliers, std::vector<int> &inlier_ids, int max_iteration = 2000, float threshold = 0.05); You can get the inlies and corresponding indices using above function.","title":"Ransac.h"},{"location":"modules/geometry/#rgbdframeh","text":"RGBDFrame.h defines the RGBDFrame data structure, which integrates the depth map and the color map, and can simplify the calling process. This data structure is mainly used to serve an RGBD SLAM system. It can save some of the extracted features, point cloud and so on, which are needed for tracking. When building an RGBD SLAM, for efficiency these contents do not need to be recalculated. Release() is used to release all saved content.","title":"RGBDFrame.h"},{"location":"modules/geometry/#kdtreeh","text":"There are two underlying implementations of KDTree, which are based on OpenCV and nanoflann respectively. You can choose any of the by settingt the pre-defined variable NANO_IMPLAMENTATION .","title":"KDTree.h"},{"location":"modules/geometry/#opencv-not-recommended","text":"The most troublesome part of calling OpenCV's KDTree is the mutual conversion between Eigen and OpenCv, and in fact, geometry::KDTree is to solve this. The following is a piece of code (from the Internet) that uses kdtree in OpenCV. int main() { //Points to build tree vector<cv::Point2f> features = { { 1,1 },{ 2, 2},{ 3, 3},{ 4, 4},{ 2, 4} }; cv::Mat source = cv::Mat(features).reshape(1); source.convertTo(source, CV_32F); cv::flann::KDTreeIndexParams indexParams(2); cv::flann::Index kdtree(source, indexParams); //Parameter int queryNum = 3;//knn vector<float> vecQuery(2);//query point vector<int> vecIndex(queryNum);//indices of the k nearest points vector<float> vecDist(queryNum);//distances cv::flann::SearchParams params(32);//max search depth vecQuery = { 3, 4}; kdtree.knnSearch(vecQuery, vecIndex, vecDist, queryNum, params); cout << \"vecDist: \" << endl; for (auto&x : vecDist) cout << x << \" \"; cout << endl; cout << \"vecIndex: \" << endl; for (auto&x : vecIndex) cout << x << \" \"; return 0; } There is some points you should be careful: 1. FLANN in OpenCV does not support double 2. In this part, the type of query point is vector<float> \uff0cif you are using cv::Mat(features[0]) , there will be a segmentation fault. 3. You need to maintain the input array all the time. So if you want to separate the building process and searching process, you need to maintain a shared matrix cloned from the origin matrix. FCLib uses a member( input_array ) in class. 4. knnSearch won't return any value, and you need to do a resize(k) for indices and distances array before searching. In radiusSearch , you don't have to resize the indices and distances array. radiusSearch will reture a number of searched points. You need to use the retured value to iterate the indices and distance array rather than indices.size() , because indices.size() is equal to max_points you set before, which is the maximum number of points you want to search. Anyway, you have no needs to worry about these problems in FCLib. In FCLib , KDTree is very simple to use. It is a template class, that is, you need to specify the dimensions of the data placed in the tree. When building the tree, directly input the PointList series defined in geometry.h (essentially the vector of the Eigen vector), which can be a dynamic vector array or a static vector array. If it is a dynamic vector array, the dimension of the dynamic vector needs to be equal to the dimension of the template. In addition, searching through RadiusSearch and KnnSearch can also be directly based on the vector of Eigen , and FCLib doesn't have any assumption about the indices array and distances array.","title":"OpenCV (Not Recommended)"},{"location":"modules/geometry/#nanoflann","text":"nanoflann is a very lightweight C++11 library, with only one hpp file. Compared with the implementation of OpenCV, nanoflann is more efficient and occupies less memory. In nanoflann, you need to declare a class for each kinds of points(such as 2D or 3D points) that needs to be searched, which contains certain functions. In FCLib, there is no need to do this. Nanoflann's radiusSearch cannot specify max_neighbors , which means it will find all points within the radius. This is not necessary in some cases and can cause waste of computing resources. FCLib modified the source code of nanoflann to get this feature (all points will be searched when max_neighbors =0). The KDTree based on nanoflann implementation has the same usage of KDTree based on OpenCV. Additionally, it supports indices of std::vector<size_t> . class SearchParameter { public: //kdtree parameter SearchParameter(int _checks = 256, float _eps = 1e-8, bool _sorted = true) { checks = _checks; eps = _eps; sorted = _sorted; } //the indices array is sorted based on the distances bool sorted = true; //EPS float eps = 1e-8; //Max search depth int checks = 256; }; template<int T = 3> class KDTree { public: KDTree(int _trees = 4) { trees = 4; } //To build KDTree from a set of high-dimensional vectors, you can choose to use a dynamic vector vector or a static vector vector void BuildTree(const geometry::PointXList &points); void BuildTree(const geometry::PointList<T> &points); //strange rules in opencv flann: //when you do knnsearch, you need to resize(k) at beginning. //when you do radius search, you need to get the returned points_count and do a resize for the results. //search points within radius for a dynamic vector or static vector void RadiusSearch(const geometry::VectorX &point, std::vector<int> &indices, std::vector<float > &dists, double radius, int max_result, const SearchParameter &sp = SearchParameter()); void RadiusSearch(const geometry::Vector<T> &point, std::vector<int> &indices, std::vector<float > &dists, double radius, int max_result, const SearchParameter sp = SearchParameter()); //search the k nearest points for a dynamic vector or static vector void KnnSearch(const geometry::VectorX &point, std::vector<int> &indices, std::vector<float > &dists, int k, const SearchParameter &sp = SearchParameter()); void KnnSearch(const geometry::Vector<T> &point, std::vector<int> &indices, std::vector<float > &dists, int k, const SearchParameter &sp = SearchParameter()); #if NANO_IMPLEMENTATION //only nanoflann based KDTree supports void RadiusSearch(const geometry::VectorX &point, std::vector<size_t> &indices, std::vector<float > &dists, double radius, int max_result, const SearchParameter &sp = SearchParameter()); void RadiusSearch(const geometry::Vector<T> &point, std::vector<size_t> &indices, std::vector<float > &dists, double radius, int max_result, const SearchParameter sp = SearchParameter()); //search the k nearest points for a dynamic vector or static vector void KnnSearch(const geometry::VectorX &point, std::vector<size_t> &indices, std::vector<float > &dists, int k, const SearchParameter &sp = SearchParameter()); void KnnSearch(const geometry::Vector<T> &point, std::vector<size_t> &indices, std::vector<float > &dists, int k, const SearchParameter &sp = SearchParameter()); #endif }; You can view example/GetLabelUsingKDTree.cpp to better understand how to use KDTree in FCLib. The input is two models: original model (without semantic information) and reference model (with semantic information). By finding the closest point through KDTree, we can obtain the original model's semantic labels.","title":"nanoflann"},{"location":"modules/integration/","text":"Generate the Model namespace: integration From tracking and optimization, now we have frames with accurate poses. How to generate a dense model? TSDF TSDF is an implicit representation of a 3D scene. The world is divided into a voxel, and each voxel stores the distance to the nearest surface. This distance is signed, and the position interpolated to 0 is the real surface If the distance is very far, it is actually useless and can be truncated directly, which is TSDF (Truncated Signed Distance Field). In FCLib, such voxel is defined as: class TSDFVoxel { public: //distance float sdf = 999; //weight float weight = 0; //color geometry::Point3 color = geometry::Point3(-1,-1,-1); }; FCLib use voxel-hashing to save the voxels that are close to surface for memory and time efficiency. \\(8\\times 8 \\times 8\\) voxels are treated as a cube, and these Cube is stored through a spatial hash list. class VoxelCube { public: std::vector<TSDFVoxel> voxels; Eigen::Vector3i cube_id; }; Through the ID of the Cube, you can easily find the location of the Cube, and then find the internal voxel. The hash map of the Cube is encapsulated in the class CubeHandler . From Frame to TSDF How to get the sdf value of a voxel through a frame? First, we calculate the position of the world coordinate system of the Voxel center through the position of voxel and the cube where it is located, and then project this position into the frame to find the corresponding depth. This depth is distance the point of the surface to the camera plane. Then SDF is defined as: $$ sdf = z(T^{-1} * p_v) - d(\\omega(T^{-1} * p_v)) $$ \\(z(\\cdot)\\) get the third value of a 3D vector, \\(d(\\cdot)\\) get the depth of an uv coordinate, \\(\\omega\\) is the reprojection function. One problem is that the three-dimensional world is infinite, so we cannot do this operation for every voxel. Here we introduce frustum, which defines an effective imaging area for the camera. Only points in the frustum are considered. The frustum is defined in Frustum.h in FCLib. For a certain frame, we only process cubes in the frustum. Even so, the number of voxels is still large. So we can sample and filter based on 8-corner of a cube. If the SDF value signs of the 8 vertices are the same, we think that there is no surface in this Cube, please refer to FlashFusion for details. A voxel's sdf may get different results for multiple frames, so we use weighted addition according to weight to get the final value. The integration of a frame is implemented by the member function of CubeHandler : void CubeHandler::IntegrateImage(const cv::Mat &depth, const cv::Mat &rgb, const geometry::TransformationMatrix & pose); void CubeHandler::IntegrateImage(const geometry::RGBDFrame &rgbd, const geometry::TransformationMatrix & pose); From TSDF to Triangle Mesh We can further extract the triangular mesh from TSDF. The most famous algorithm is Marching Cubes . In Marching Cubes, the extraction of surface is done through cubes(not the cube we mentioned in CubeHandler). A cube has 8 vertices and 8 sdf values respectively. And the sdf value of each vertex may be greater than 0 or less than 0, so there are \\(8 \\times 8 \\times 8\\) possible situations. However, there are only 15 basic cases listed as following: 256 cases have been enumerated to form a mapping table to store the edges where the triangle vertices is interpolated, so given any cube like this, the triangle can be extracted according to the mapping table. In TSDF, 8 Voxel can be regarded as 8 vertices of a cube, and TSDF to Mesh is performed in this way. Mesh extraction is also implemented by the member function of CubeHandler : void CubeHandler::ExtractTriangleMesh(geometry::TriangleMesh &mesh); Transformation of TSDF Compared with point cloud and triangular mesh, the rotation and translation of TSDF is slightly different. The world are divided into voxels, and after the rotation and translation, the transformed voxel center coordinates become real numbers and need to be remapped to the voxels in the world coordinate system. If the transformed voxel is only mapped to the nearest voxel, it will cause inaccurate results, and may produce holes and missing. An 8-neighbor filter can provide accurate results. In CubeHandler , we provide these two methods of Transformation for comparison(It will be more obvious if the voxel resolution is higher): //Map to the nearest voxel std::shared_ptr<CubeHandler> TransformNearest(const geometry::TransformationMatrix &trans) const; //Map to 8 neighbor voxel, use weighted addition to get the final value std::shared_ptr<CubeHandler> Transform(const geometry::TransformationMatrix &trans) const; 8-neighbor(voxel resolution: 0.00625) neareast(voxel resolution: 0.00625) It should be noted that the FCLib implements the most primitive Marching Cubes algorithm. The obtained Mesh is often very dense, you can use ClusteringSimplify to aggregate the points which are close to one point, and the normal vector can be calculated by ComputeNormals .","title":"Generate Model"},{"location":"modules/integration/#generate-the-model","text":"namespace: integration From tracking and optimization, now we have frames with accurate poses. How to generate a dense model?","title":"Generate the Model"},{"location":"modules/integration/#tsdf","text":"TSDF is an implicit representation of a 3D scene. The world is divided into a voxel, and each voxel stores the distance to the nearest surface. This distance is signed, and the position interpolated to 0 is the real surface If the distance is very far, it is actually useless and can be truncated directly, which is TSDF (Truncated Signed Distance Field). In FCLib, such voxel is defined as: class TSDFVoxel { public: //distance float sdf = 999; //weight float weight = 0; //color geometry::Point3 color = geometry::Point3(-1,-1,-1); }; FCLib use voxel-hashing to save the voxels that are close to surface for memory and time efficiency. \\(8\\times 8 \\times 8\\) voxels are treated as a cube, and these Cube is stored through a spatial hash list. class VoxelCube { public: std::vector<TSDFVoxel> voxels; Eigen::Vector3i cube_id; }; Through the ID of the Cube, you can easily find the location of the Cube, and then find the internal voxel. The hash map of the Cube is encapsulated in the class CubeHandler .","title":"TSDF"},{"location":"modules/integration/#from-frame-to-tsdf","text":"How to get the sdf value of a voxel through a frame? First, we calculate the position of the world coordinate system of the Voxel center through the position of voxel and the cube where it is located, and then project this position into the frame to find the corresponding depth. This depth is distance the point of the surface to the camera plane. Then SDF is defined as: $$ sdf = z(T^{-1} * p_v) - d(\\omega(T^{-1} * p_v)) $$ \\(z(\\cdot)\\) get the third value of a 3D vector, \\(d(\\cdot)\\) get the depth of an uv coordinate, \\(\\omega\\) is the reprojection function. One problem is that the three-dimensional world is infinite, so we cannot do this operation for every voxel. Here we introduce frustum, which defines an effective imaging area for the camera. Only points in the frustum are considered. The frustum is defined in Frustum.h in FCLib. For a certain frame, we only process cubes in the frustum. Even so, the number of voxels is still large. So we can sample and filter based on 8-corner of a cube. If the SDF value signs of the 8 vertices are the same, we think that there is no surface in this Cube, please refer to FlashFusion for details. A voxel's sdf may get different results for multiple frames, so we use weighted addition according to weight to get the final value. The integration of a frame is implemented by the member function of CubeHandler : void CubeHandler::IntegrateImage(const cv::Mat &depth, const cv::Mat &rgb, const geometry::TransformationMatrix & pose); void CubeHandler::IntegrateImage(const geometry::RGBDFrame &rgbd, const geometry::TransformationMatrix & pose);","title":"From Frame to TSDF"},{"location":"modules/integration/#from-tsdf-to-triangle-mesh","text":"We can further extract the triangular mesh from TSDF. The most famous algorithm is Marching Cubes . In Marching Cubes, the extraction of surface is done through cubes(not the cube we mentioned in CubeHandler). A cube has 8 vertices and 8 sdf values respectively. And the sdf value of each vertex may be greater than 0 or less than 0, so there are \\(8 \\times 8 \\times 8\\) possible situations. However, there are only 15 basic cases listed as following: 256 cases have been enumerated to form a mapping table to store the edges where the triangle vertices is interpolated, so given any cube like this, the triangle can be extracted according to the mapping table. In TSDF, 8 Voxel can be regarded as 8 vertices of a cube, and TSDF to Mesh is performed in this way. Mesh extraction is also implemented by the member function of CubeHandler : void CubeHandler::ExtractTriangleMesh(geometry::TriangleMesh &mesh);","title":"From TSDF to Triangle Mesh"},{"location":"modules/integration/#transformation-of-tsdf","text":"Compared with point cloud and triangular mesh, the rotation and translation of TSDF is slightly different. The world are divided into voxels, and after the rotation and translation, the transformed voxel center coordinates become real numbers and need to be remapped to the voxels in the world coordinate system. If the transformed voxel is only mapped to the nearest voxel, it will cause inaccurate results, and may produce holes and missing. An 8-neighbor filter can provide accurate results. In CubeHandler , we provide these two methods of Transformation for comparison(It will be more obvious if the voxel resolution is higher): //Map to the nearest voxel std::shared_ptr<CubeHandler> TransformNearest(const geometry::TransformationMatrix &trans) const; //Map to 8 neighbor voxel, use weighted addition to get the final value std::shared_ptr<CubeHandler> Transform(const geometry::TransformationMatrix &trans) const; 8-neighbor(voxel resolution: 0.00625) neareast(voxel resolution: 0.00625) It should be noted that the FCLib implements the most primitive Marching Cubes algorithm. The obtained Mesh is often very dense, you can use ClusteringSimplify to aggregate the points which are close to one point, and the normal vector can be calculated by ComputeNormals .","title":"Transformation of TSDF"},{"location":"modules/lcdetection/","text":"Loop Closure Detection namespace: lcdetection Due to noise, the tracking results will definitely have some errors, which means that after scanning a circle, they may not return to the original point. Therefore, ghosts will appear in final model. Loop closure detection is used to check if the current scanned image and the previous scan are the same place by compute the similarity of image, and provide constraints for the global pose optimization to global consistency. There are many loop closure detection methods, including 2D and 3D. At present, FCLib only introduces MILD , which is a 2D loop closure detection based on multi-index hash. We use MILD to provide a series of similar frame, and odometry::SparseTracking will be further used to check if they are the correct correspondence. Because loop closure detection removes most of the outlier, the time consumed can be greatly reduced. FCLib encapsulates MILD into a class MildLCDetector , the main member functions are: //Find possible repeated scenes for the current frame, input is the current frame //The output is a collection of IDs of frames in the database that may be repeated scenes void SelectCandidates(const geometry::RGBDFrame &frame, std::vector<int> &candidates); //insert a frame into database void Insert(const geometry::RGBDFrame &frame); More loop closure detection methods may be added later.","title":"Loop Closure Detection"},{"location":"modules/lcdetection/#loop-closure-detection","text":"namespace: lcdetection Due to noise, the tracking results will definitely have some errors, which means that after scanning a circle, they may not return to the original point. Therefore, ghosts will appear in final model. Loop closure detection is used to check if the current scanned image and the previous scan are the same place by compute the similarity of image, and provide constraints for the global pose optimization to global consistency. There are many loop closure detection methods, including 2D and 3D. At present, FCLib only introduces MILD , which is a 2D loop closure detection based on multi-index hash. We use MILD to provide a series of similar frame, and odometry::SparseTracking will be further used to check if they are the correct correspondence. Because loop closure detection removes most of the outlier, the time consumed can be greatly reduced. FCLib encapsulates MILD into a class MildLCDetector , the main member functions are: //Find possible repeated scenes for the current frame, input is the current frame //The output is a collection of IDs of frames in the database that may be repeated scenes void SelectCandidates(const geometry::RGBDFrame &frame, std::vector<int> &candidates); //insert a frame into database void Insert(const geometry::RGBDFrame &frame); More loop closure detection methods may be added later.","title":"Loop Closure Detection"},{"location":"modules/odometry/","text":"Visual Odometry namespace: odometry The visual odometer here specifically refers to the RGBD visual odometer. The function of the visual odometer is to estimate the camera movement between two frames, which is the first step of SLAM. Two visual odometries are implemented in FCLib, which are widely used: Sparse matching based on feature points, and dense matching based on optical flow. The usage is very simple: std::shared_ptr<SparseTrackingResult> SparseTracking(const cv::Mat &source_color, const cv::Mat &target_color, const cv::Mat &source_depth, const cv::Mat &target_depth); std::shared_ptr<SparseTrackingResult> SparseTracking(geometry::RGBDFrame &source_frame, geometry::RGBDFrame &target_frame); std::shared_ptr<SparseTrackingResult> SparseTrackingMILD(const cv::Mat &source_color, const cv::Mat &target_color, const cv::Mat &source_depth, const cv::Mat &target_depth); std::shared_ptr<SparseTrackingResult> SparseTrackingMILD(geometry::RGBDFrame &source_frame, geometry::RGBDFrame &target_frame); std::shared_ptr<DenseTrackingResult> DenseTracking(const cv::Mat &source_color, const cv::Mat &target_color, const cv::Mat &source_depth, const cv::Mat &target_depth, const geometry::TransformationMatrix &initial_T, int term_type = 0); std::shared_ptr<DenseTrackingResult> DenseTracking(geometry::RGBDFrame &source_frame, geometry::RGBDFrame &target_frame, const geometry::TransformationMatrix &initial_T, int term_type = 0); You can choose to use a pair of rgb and depth to tracking, or a pair of geometry::RGBDFrame . The result retunred is a shared_ptr, which pointer an object of odometry::SparseTrackingResult or odometry::DenseTrackingResult . They are defined as following\uff1a class SparseTrackingResult { public: //relative pose geometry::TransformationMatrix T; //indices of inliers geometry::FMatchSet correspondence_set_index; //nliers geometry::PointCorrespondenceSet correspondence_set; //rmse double rmse = 1e6; //if the tracking is successful bool tracking_success; }; class DenseTrackingResult { public: //relative pose geometry::TransformationMatrix T; //indices of inliers geometry::PixelCorrespondenceSet pixel_correspondence_set; //inliers geometry::PointCorrespondenceSet correspondence_set; //rmse double rmse = 1e6; //if the tracking is successful bool tracking_success; }; The main difference is that the ID types of the matching points finally obtained by the two are different. The sparse tracking feature point ID is one-dimensional. For example, a pair of correctly matched feature points is \\((501, 308)\\) , which refers to the 501st feature point of source frame and the 308th feature point of target frame. The ID of the point for dense tracking is two-dimensional. The result is for example: \\([(501, 308), (489, 311)]\\) , which refers to a pair of 3D points of source frame and target frame in pixel \\((501, 308)\\) and \\((489, 311)\\) . Of course, the results of dense tracking can be integrated into the same form as the results of sparse tracking, but FCLib believes that this format is closer to the essence of the algorithm, because it reveals a piece of information: sparse tracking is based on the extracted feature points, while dense tracking is pixel by pixel. SparseTracking The basic steps of SparseTracking are as follows: - Extract feature points - Feature point matching - Outlier filtering - Compute the transformation matrix FCLib extract ORB feature, which is a fast binary feature. For feature matching, FCLib introduce two methods: BFMatcher(brutal fource) based on OpenCV, and Sparse Match introduced in MILD (Open Source Library). Refer to Beyond SIFT Using Binary features in Loop Closure Detection for details, which is downloaded in relative_paper/sparse_match.pdf . This method is packaged in class SparseMatcher , the main member functions are listed as following: //Match void Match(const geometry::Descriptor &source_descriptors, const geometry::Descriptor &target_descriptors, geometry::DMatchSet & matches); //Refine the matching results after matching //Need 3D points void RefineMatches(const geometry::Descriptor &source_descriptors, const geometry::Point3List &source_local_points, const geometry::KeyPointSet &source_keypoints, const geometry::KeyPointSet &target_keypoints, const geometry::TransformationMatrix & H,geometry::DMatchSet & matches); There are also several methods for outlier filtering, odometry::outlier_filter provides several common methods. //Delete matches with a distance greater than n times the minimum distance (Naive) void MinDistance(const geometry::KeyPointSet &source_keypoints, const geometry::KeyPointSet &target_keypoints,geometry::DMatchSet & matches); //Only keep the matches with a larger ratio between the sub-optimal distance and the optimal distance (the optimal result of a correct match should be much better than the sub-optimal result) void KnnMatch(const geometry::Descriptor &source, geometry::Descriptor & target, const cv::BFMatcher &matcher, std::vector< cv::DMatch > &matches, float minRatio = 1.5); //Use ransic to find the Homography matrix, keep the inliers void Ransac(const geometry::KeyPointSet &source_keypoints, const geometry::KeyPointSet &target_keypoints,geometry::DMatchSet & matches); //For the currently checked matching pair, randomly select several other matches to see if they are consistent //Wrong matching is difficult to find other consistent matching pairs void RanSaPC(const geometry::Point3List &source_local_points, const geometry::Point3List &target_local_points, std::default_random_engine &engine, std::vector< cv::DMatch > &init_matches); FCLib names the tracking function using MILD as SparseTrackingMILD . Generally speaking, it has better performance than SparseTracking and can get more The right inliers. DenseTracking For details of DenseTracking, refer to Real-Time Visual Odometry from Dense RGB-D Images , you can also find the paper from related_paper/steinbruecker_sturm_cremers_iccv11.pdf . The above paper is only for tracking the intensity of the color map, that is, the optical flow method. However it is easy to extend to track the depth of the depth map. In FCLib, you can choose to use color or depth term, or hybrid terms by changing the last parameter term_type : if(term_type == 0 )//hybrid std::cout<<BLUE<<\"[DEBUG]::Using hybrid term\"<<RESET<<std::endl; else if(term_type == 1)//color std::cout<<BLUE<<\"[DEBUG]::Using photo term\"<<RESET<<std::endl; else if(term_type == 2)//depth std::cout<<BLUE<<\"[DEBUG]::Using geometry term\"<<RESET<<std::endl; Steps of DenseTracking: - Calculate the corresponding point in target from for each pixel of the source frame according to the current pose - Find all inliers of corresponding points - According to these points, compute the Jacobian matrix for inliers and update the pose - Repeat the above process It can be seen that DenseTracking requires an initial pose. If there is no initial estimation, it is necessary to assume that the relative poses of the two frames are very close. This process is a bit like PointToPoint's ICP , the difference is that the optimized objective function, so PointToPoint needs to find the closest point instead of using reprojection. The process of finding the closest point is time-consuming, so dense tracking is much faster than ICP. In the specific implementation, the image pyramid will be used to improve the efficiency. Comparison SparseTracking has fewer feature points, is faster, and does not require initial pose. However, it only considers visual information, and cannot be used in some places with less texture or poor color map. DenseTracking is slower and requires an initial pose. But it is more robust, and can be used even in the case of only depth or only color map. It is a good backup solution when SparseTracking fails. RGBDFrame It can be seen that there are two functions for each type of tracking: they act directly on rgb and depth or act on RGBDFrame : std::shared_ptr<SparseTrackingResult> SparseTracking(const cv::Mat &source_color, const cv::Mat &target_color, const cv::Mat &source_depth, const cv::Mat &target_depth); std::shared_ptr<SparseTrackingResult> SparseTracking(geometry::RGBDFrame &source_frame, geometry::RGBDFrame &target_frame); The difference is that geometry::RGBDFrame will save intermediate information, which can avoid repeated calculation when building a SLAM system: //part of RGBDFrame /*for sparse odometry*/ geometry::Descriptor descriptor; geometry::KeyPointSet keypoints; PointCloud feature_pcd;//for optimization /*for dense odometry*/ cv::Mat gray;//gray image of rgb cv::Mat depth32f;//refined depth std::vector<cv::Mat> color_pyramid; std::vector<cv::Mat> depth_pyramid; std::vector<cv::Mat> color_dx_pyramid; std::vector<cv::Mat> color_dy_pyramid; std::vector<cv::Mat> depth_dx_pyramid; std::vector<cv::Mat> depth_dy_pyramid; std::vector<geometry::ImageXYZ> image_xyz; std::vector<geometry::ImageXYZ> image_xyz can be considered as 3D image with a correspondig 3D point for each pixel. We can use 2D pixel coordinate to find the point, which is the reason why the corresponding point ID in DenseTrackingResult is two-dimensional.","title":"Odometry"},{"location":"modules/odometry/#visual-odometry","text":"namespace: odometry The visual odometer here specifically refers to the RGBD visual odometer. The function of the visual odometer is to estimate the camera movement between two frames, which is the first step of SLAM. Two visual odometries are implemented in FCLib, which are widely used: Sparse matching based on feature points, and dense matching based on optical flow. The usage is very simple: std::shared_ptr<SparseTrackingResult> SparseTracking(const cv::Mat &source_color, const cv::Mat &target_color, const cv::Mat &source_depth, const cv::Mat &target_depth); std::shared_ptr<SparseTrackingResult> SparseTracking(geometry::RGBDFrame &source_frame, geometry::RGBDFrame &target_frame); std::shared_ptr<SparseTrackingResult> SparseTrackingMILD(const cv::Mat &source_color, const cv::Mat &target_color, const cv::Mat &source_depth, const cv::Mat &target_depth); std::shared_ptr<SparseTrackingResult> SparseTrackingMILD(geometry::RGBDFrame &source_frame, geometry::RGBDFrame &target_frame); std::shared_ptr<DenseTrackingResult> DenseTracking(const cv::Mat &source_color, const cv::Mat &target_color, const cv::Mat &source_depth, const cv::Mat &target_depth, const geometry::TransformationMatrix &initial_T, int term_type = 0); std::shared_ptr<DenseTrackingResult> DenseTracking(geometry::RGBDFrame &source_frame, geometry::RGBDFrame &target_frame, const geometry::TransformationMatrix &initial_T, int term_type = 0); You can choose to use a pair of rgb and depth to tracking, or a pair of geometry::RGBDFrame . The result retunred is a shared_ptr, which pointer an object of odometry::SparseTrackingResult or odometry::DenseTrackingResult . They are defined as following\uff1a class SparseTrackingResult { public: //relative pose geometry::TransformationMatrix T; //indices of inliers geometry::FMatchSet correspondence_set_index; //nliers geometry::PointCorrespondenceSet correspondence_set; //rmse double rmse = 1e6; //if the tracking is successful bool tracking_success; }; class DenseTrackingResult { public: //relative pose geometry::TransformationMatrix T; //indices of inliers geometry::PixelCorrespondenceSet pixel_correspondence_set; //inliers geometry::PointCorrespondenceSet correspondence_set; //rmse double rmse = 1e6; //if the tracking is successful bool tracking_success; }; The main difference is that the ID types of the matching points finally obtained by the two are different. The sparse tracking feature point ID is one-dimensional. For example, a pair of correctly matched feature points is \\((501, 308)\\) , which refers to the 501st feature point of source frame and the 308th feature point of target frame. The ID of the point for dense tracking is two-dimensional. The result is for example: \\([(501, 308), (489, 311)]\\) , which refers to a pair of 3D points of source frame and target frame in pixel \\((501, 308)\\) and \\((489, 311)\\) . Of course, the results of dense tracking can be integrated into the same form as the results of sparse tracking, but FCLib believes that this format is closer to the essence of the algorithm, because it reveals a piece of information: sparse tracking is based on the extracted feature points, while dense tracking is pixel by pixel.","title":"Visual Odometry"},{"location":"modules/odometry/#sparsetracking","text":"The basic steps of SparseTracking are as follows: - Extract feature points - Feature point matching - Outlier filtering - Compute the transformation matrix FCLib extract ORB feature, which is a fast binary feature. For feature matching, FCLib introduce two methods: BFMatcher(brutal fource) based on OpenCV, and Sparse Match introduced in MILD (Open Source Library). Refer to Beyond SIFT Using Binary features in Loop Closure Detection for details, which is downloaded in relative_paper/sparse_match.pdf . This method is packaged in class SparseMatcher , the main member functions are listed as following: //Match void Match(const geometry::Descriptor &source_descriptors, const geometry::Descriptor &target_descriptors, geometry::DMatchSet & matches); //Refine the matching results after matching //Need 3D points void RefineMatches(const geometry::Descriptor &source_descriptors, const geometry::Point3List &source_local_points, const geometry::KeyPointSet &source_keypoints, const geometry::KeyPointSet &target_keypoints, const geometry::TransformationMatrix & H,geometry::DMatchSet & matches); There are also several methods for outlier filtering, odometry::outlier_filter provides several common methods. //Delete matches with a distance greater than n times the minimum distance (Naive) void MinDistance(const geometry::KeyPointSet &source_keypoints, const geometry::KeyPointSet &target_keypoints,geometry::DMatchSet & matches); //Only keep the matches with a larger ratio between the sub-optimal distance and the optimal distance (the optimal result of a correct match should be much better than the sub-optimal result) void KnnMatch(const geometry::Descriptor &source, geometry::Descriptor & target, const cv::BFMatcher &matcher, std::vector< cv::DMatch > &matches, float minRatio = 1.5); //Use ransic to find the Homography matrix, keep the inliers void Ransac(const geometry::KeyPointSet &source_keypoints, const geometry::KeyPointSet &target_keypoints,geometry::DMatchSet & matches); //For the currently checked matching pair, randomly select several other matches to see if they are consistent //Wrong matching is difficult to find other consistent matching pairs void RanSaPC(const geometry::Point3List &source_local_points, const geometry::Point3List &target_local_points, std::default_random_engine &engine, std::vector< cv::DMatch > &init_matches); FCLib names the tracking function using MILD as SparseTrackingMILD . Generally speaking, it has better performance than SparseTracking and can get more The right inliers.","title":"SparseTracking"},{"location":"modules/odometry/#densetracking","text":"For details of DenseTracking, refer to Real-Time Visual Odometry from Dense RGB-D Images , you can also find the paper from related_paper/steinbruecker_sturm_cremers_iccv11.pdf . The above paper is only for tracking the intensity of the color map, that is, the optical flow method. However it is easy to extend to track the depth of the depth map. In FCLib, you can choose to use color or depth term, or hybrid terms by changing the last parameter term_type : if(term_type == 0 )//hybrid std::cout<<BLUE<<\"[DEBUG]::Using hybrid term\"<<RESET<<std::endl; else if(term_type == 1)//color std::cout<<BLUE<<\"[DEBUG]::Using photo term\"<<RESET<<std::endl; else if(term_type == 2)//depth std::cout<<BLUE<<\"[DEBUG]::Using geometry term\"<<RESET<<std::endl; Steps of DenseTracking: - Calculate the corresponding point in target from for each pixel of the source frame according to the current pose - Find all inliers of corresponding points - According to these points, compute the Jacobian matrix for inliers and update the pose - Repeat the above process It can be seen that DenseTracking requires an initial pose. If there is no initial estimation, it is necessary to assume that the relative poses of the two frames are very close. This process is a bit like PointToPoint's ICP , the difference is that the optimized objective function, so PointToPoint needs to find the closest point instead of using reprojection. The process of finding the closest point is time-consuming, so dense tracking is much faster than ICP. In the specific implementation, the image pyramid will be used to improve the efficiency.","title":"DenseTracking"},{"location":"modules/odometry/#comparison","text":"SparseTracking has fewer feature points, is faster, and does not require initial pose. However, it only considers visual information, and cannot be used in some places with less texture or poor color map. DenseTracking is slower and requires an initial pose. But it is more robust, and can be used even in the case of only depth or only color map. It is a good backup solution when SparseTracking fails.","title":"Comparison"},{"location":"modules/odometry/#rgbdframe","text":"It can be seen that there are two functions for each type of tracking: they act directly on rgb and depth or act on RGBDFrame : std::shared_ptr<SparseTrackingResult> SparseTracking(const cv::Mat &source_color, const cv::Mat &target_color, const cv::Mat &source_depth, const cv::Mat &target_depth); std::shared_ptr<SparseTrackingResult> SparseTracking(geometry::RGBDFrame &source_frame, geometry::RGBDFrame &target_frame); The difference is that geometry::RGBDFrame will save intermediate information, which can avoid repeated calculation when building a SLAM system: //part of RGBDFrame /*for sparse odometry*/ geometry::Descriptor descriptor; geometry::KeyPointSet keypoints; PointCloud feature_pcd;//for optimization /*for dense odometry*/ cv::Mat gray;//gray image of rgb cv::Mat depth32f;//refined depth std::vector<cv::Mat> color_pyramid; std::vector<cv::Mat> depth_pyramid; std::vector<cv::Mat> color_dx_pyramid; std::vector<cv::Mat> color_dy_pyramid; std::vector<cv::Mat> depth_dx_pyramid; std::vector<cv::Mat> depth_dy_pyramid; std::vector<geometry::ImageXYZ> image_xyz; std::vector<geometry::ImageXYZ> image_xyz can be considered as 3D image with a correspondig 3D point for each pixel. We can use 2D pixel coordinate to find the point, which is the reason why the corresponding point ID in DenseTrackingResult is two-dimensional.","title":"RGBDFrame"},{"location":"modules/optimization/","text":"Optimization namespace: optimization As mentioned in Loop Detection , tracking will have accumulated errors, and we need closed-loop detection to provide constraints to optimize and eliminate errors. Optimization is the content of this part. Back-end optimization generally uses an optimizer, libraries like g2o, ceres are very good, but unfortunately the author is only familiar with Eigen. The optimization part is implemented by Eigen, which should be more helpful for learning. FCLib implements two Two back-end optimization methods: Fast Bundle Adjustment and Bundle Adjustment. Fast BundleAdjustment Frame correspondence is defined in class Correspondence : class Correspondence { public: int source_id = -1; int target_id = -1; float average_disparity = 1e6; geometry::PointCorrespondenceSet correspondence_set; }; A Frame Correspondence means a pair of matched frames, where the source id is the id of the source frame, and the target id is the id of the target frame in the optimized variable. And average_disparity is used to measure the distance of these matching points in a 2-dimensional picture, and to determine whether to start a new keyframe. geometry::PointCorrespondenceSet is the set of matching points contained in this pair of matching frames. Fast BundleAdjustment (FBA) is faster than the traditional Bundle Adjustment. In fact, FBA is called only in FCLib and does not have a very standard name. The difference between it and BundleAdjustment is the optimization variables and objective functions. FBA only optimizes the camera pose, The optimization objective function is: \\[ E(T) = \\sum_{i}^{\\vert\\Omega\\vert}\\sum_{j}^{\\vert C_{i}\\vert}\\Vert T_sp_{s(j)} - T_tp_{t(j)} \\Vert^2 \\] \\(\\Omega\\) refers to all frame correspondences, \\(C_{i}\\) refers to the \\(i\\) frame correspondence, which is composed of multiple point correspondences. \\(T_s and T_t\\) represent the poses of the source frame and target frame in the frame correspondence, respectively. \\(p_(s(j))\\) represents the source point in the \\(j\\) th point correspondence. For the optimization of this part, you can refer to the article fastgo , you can also find it in related_paper/fastgo.pdf . This article contains more optimization techniques to improve system efficiency, while FCLib only implements the basic parts. The functions are as follows: void FastBA(const std::vector<Correspondence> &correspondences, geometry::SE3List &poses, int max_iteration =5); Bundle Adjutment n addition to optimizing the pose, the traditional BundleAdjustment (BA) also optimizes the 3D point coordinates in the world coordinate system, and the optimization goal is the 2D reprojection error. Generally speaking, the input of the traditional BA is the 3d point in the world coordinate system, camera poses, the projected 2D coordinates of 3D points in each frame. The objective function is: \\[ E(T, p) = \\sum_i^{N_f}\\sum_{j}^{N_p}\\Vert u_{i,j} - \\omega(T_i^{-1} p_j) \\Vert, \\] \\(N_f\\) is the number of camera poses (frame), \\(N_p\\) is the number of world coordinate points, and \\(u_{i,j}\\) is the actual projection position (uv coordinates during feature matching), and $\\omega $ is a function for calculating reprojection. For this optimization, in addition to optimizing the camera pose, it also optimizes the position of the point in the world, which means the optimization of the noise in the projection process. However, because the number of world points is often very large, the final optimization matrix dimension is tens of thousands, even millions, which highly limits the solving speed. BA is a very classic method, which was proposed long before the emergence of SLAM, and there are many variants of it, acceleration methods. This part is just an implementation of the basic idea. void BundleAdjustment(geometry::Point3List &world_points, std::vector<ProjectedPointsOnFrame> &projected_points, geometry::SE3List &poses, const camera::PinholeCamera &camera, int max_iteration = 20); When implementing FastBA, FCLib uses the Gauss-Newton optimization method, and BA must use the Levenberg-Marquardt algorithm, because it is easy to appear singular matrices, which causes Gauss-Newton to become very fragile and makes the result unable to converge. For the detailed derivation of BA, refer to related_paper/BARevisited.pdf . Look the examples: bundle adjustment of two frames .","title":"Optimization"},{"location":"modules/optimization/#optimization","text":"namespace: optimization As mentioned in Loop Detection , tracking will have accumulated errors, and we need closed-loop detection to provide constraints to optimize and eliminate errors. Optimization is the content of this part. Back-end optimization generally uses an optimizer, libraries like g2o, ceres are very good, but unfortunately the author is only familiar with Eigen. The optimization part is implemented by Eigen, which should be more helpful for learning. FCLib implements two Two back-end optimization methods: Fast Bundle Adjustment and Bundle Adjustment.","title":"Optimization"},{"location":"modules/optimization/#fast-bundleadjustment","text":"Frame correspondence is defined in class Correspondence : class Correspondence { public: int source_id = -1; int target_id = -1; float average_disparity = 1e6; geometry::PointCorrespondenceSet correspondence_set; }; A Frame Correspondence means a pair of matched frames, where the source id is the id of the source frame, and the target id is the id of the target frame in the optimized variable. And average_disparity is used to measure the distance of these matching points in a 2-dimensional picture, and to determine whether to start a new keyframe. geometry::PointCorrespondenceSet is the set of matching points contained in this pair of matching frames. Fast BundleAdjustment (FBA) is faster than the traditional Bundle Adjustment. In fact, FBA is called only in FCLib and does not have a very standard name. The difference between it and BundleAdjustment is the optimization variables and objective functions. FBA only optimizes the camera pose, The optimization objective function is: \\[ E(T) = \\sum_{i}^{\\vert\\Omega\\vert}\\sum_{j}^{\\vert C_{i}\\vert}\\Vert T_sp_{s(j)} - T_tp_{t(j)} \\Vert^2 \\] \\(\\Omega\\) refers to all frame correspondences, \\(C_{i}\\) refers to the \\(i\\) frame correspondence, which is composed of multiple point correspondences. \\(T_s and T_t\\) represent the poses of the source frame and target frame in the frame correspondence, respectively. \\(p_(s(j))\\) represents the source point in the \\(j\\) th point correspondence. For the optimization of this part, you can refer to the article fastgo , you can also find it in related_paper/fastgo.pdf . This article contains more optimization techniques to improve system efficiency, while FCLib only implements the basic parts. The functions are as follows: void FastBA(const std::vector<Correspondence> &correspondences, geometry::SE3List &poses, int max_iteration =5);","title":"Fast BundleAdjustment"},{"location":"modules/optimization/#bundle-adjutment","text":"n addition to optimizing the pose, the traditional BundleAdjustment (BA) also optimizes the 3D point coordinates in the world coordinate system, and the optimization goal is the 2D reprojection error. Generally speaking, the input of the traditional BA is the 3d point in the world coordinate system, camera poses, the projected 2D coordinates of 3D points in each frame. The objective function is: \\[ E(T, p) = \\sum_i^{N_f}\\sum_{j}^{N_p}\\Vert u_{i,j} - \\omega(T_i^{-1} p_j) \\Vert, \\] \\(N_f\\) is the number of camera poses (frame), \\(N_p\\) is the number of world coordinate points, and \\(u_{i,j}\\) is the actual projection position (uv coordinates during feature matching), and $\\omega $ is a function for calculating reprojection. For this optimization, in addition to optimizing the camera pose, it also optimizes the position of the point in the world, which means the optimization of the noise in the projection process. However, because the number of world points is often very large, the final optimization matrix dimension is tens of thousands, even millions, which highly limits the solving speed. BA is a very classic method, which was proposed long before the emergence of SLAM, and there are many variants of it, acceleration methods. This part is just an implementation of the basic idea. void BundleAdjustment(geometry::Point3List &world_points, std::vector<ProjectedPointsOnFrame> &projected_points, geometry::SE3List &poses, const camera::PinholeCamera &camera, int max_iteration = 20); When implementing FastBA, FCLib uses the Gauss-Newton optimization method, and BA must use the Levenberg-Marquardt algorithm, because it is easy to appear singular matrices, which causes Gauss-Newton to become very fragile and makes the result unable to converge. For the detailed derivation of BA, refer to related_paper/BARevisited.pdf . Look the examples: bundle adjustment of two frames .","title":"Bundle Adjutment"},{"location":"modules/registration/","text":"Point Cloud Registration namespace: registration The task of point cloud registration is very similar to the visual odometer, except that the input of the visual odometer is a pair of frames and camera intrinsics, and the input of the point cloud registration is two point clouds without other conditiosn, such as camera parameters. So it is impossible to use 2D reprojection for point cloud registration. Generally speaking, the scale of the point cloud used for registration is much larger than frame, which can be a model extracted from dozens of frames, or even a model of two rooms. For point cloud registration, there are also two different ways just the spase tracking and dense tracking in visual odometry: ICP based on dense points and Global Registration based on 3D features. Global Registration Global Registration does not require an initial pose, the basic steps are: - Extract 3D features - Feature matching - Outlier filtering - Calculate transformation matrix The methods are as following: std::shared_ptr<RegistrationResult> RansacRegistration(const geometry::PointCloud &source_pcd, const geometry::PointCloud &target_pcd, const RANSACParameter &r_para = RANSACParameter()); std::shared_ptr<RegistrationResult> RansacRegistration(const geometry::PointCloud &source_feature_pcd, const geometry::PointCloud &target_feature_pcd, const FeatureSet & source_features, const FeatureSet & target_features, const RANSACParameter &r_para); The difference between the above two functions is that the first is to directly register the two point clouds, and the second is to register the feature points, which means that the step of extracting 3D features is skipped. When building a system, saving the 3D features can avoid repeated calculation and improve efficiency. Extracting 3D features are generally implemented in libraries such as PCL or Open3D, but such libraries are too large. OpenCV does not implement 3D feature extraction, so FCLib implements this part. FPFH features can be calculated by the following function : std::tuple<geometry::PointCloud, FeatureSet> DownSampleAndExtractFeature(const geometry::PointCloud &pcd, const RANSACParameter &r_para = RANSACParameter()); The above function input is the point cloud and related parameters, and returns the extracted feature point position and features (33-dimensional vector). The we can perform knn search for the nearest point to get the matching result. Outlier filtering are quiet similar to the methods introduced in SparseTracking , so I won\u2019t introduce them here. Finally, the transformation matrix and inliers are calculated by RANSAC . ICP ICP (iterative closest point) is a very classic algorithm with many variants. FCLib implements two basic ICPs, one is point-to-point and the other is point-to-plane. PointToPoint PointToPoint optimizes the distance from point to point. The optimized objective function is: \\(\\Vert T \\cdot p_s-p_t\\Vert^2\\) . Through OpenCV's KDtree to find the closest point, then the best transformation \\(T\\) can be obtained directly by the geometry::EstimateRigidTransformation (which can also be optimized by computing the Jacobian matrix). Then re-find the nearest point according to the new \\(T\\) , and iterate the process until convergence. PointToPlane PointToPlane optimizes the distance from point to plane, so it needs the vector normal of target point cloud. The equation of the plane whose normal vector is \\(n_t\\) and passing through \\(p_t\\) is $$ n_t^T (\\cdot p - p_t) = 0, $$ \\(n_t^T (\\cdot p - p_t)\\) is the distance from point to plane. So the objective function is: \\(\\Vert n_t^T\\cdot(T\\cdot p_s - p_t)\\Vert^2\\) . For the linearization of this cost function, you can view related_paper/lowk_point-to-plane_icp_techrep.pdf . The new \\(T\\) is obtained through computing Jacobian matrix and optimization, and then the closest point is re-finded according to the new \\(T\\) . Iterate this process until convergence . Both experimental and theoretical analysis prove that PointToPlane has a faster convergence rate than PointToPoint . The method is defined as following: std::shared_ptr<RegistrationResult> PointToPoint(const geometry::PointCloud &source, const geometry::PointCloud &target, const geometry::TransformationMatrix &init_T = geometry::TransformationMatrix::Identity(), const ICPParameter &icp_para = ICPParameter() ); std::shared_ptr<RegistrationResult> PointToPlane(const geometry::PointCloud &source, const geometry::PointCloud &target, const geometry::TransformationMatrix &init_T = geometry::TransformationMatrix::Identity(), const ICPParameter &icp_para = ICPParameter() ); icp_para are parameters of ICP, using the default value is a good choice. RegistrationResult is defined as following: class RegistrationResult { public: //relative transformation geometry::TransformationMatrix T; //the id of inliers geometry::FMatchSet correspondence_set_index; //inliers geometry::PointCorrespondenceSet correspondence_set; //rmse double rmse; }; In this case, it is left to the user to determine whether the registration is successful.","title":"Point Cloud Registration"},{"location":"modules/registration/#point-cloud-registration","text":"namespace: registration The task of point cloud registration is very similar to the visual odometer, except that the input of the visual odometer is a pair of frames and camera intrinsics, and the input of the point cloud registration is two point clouds without other conditiosn, such as camera parameters. So it is impossible to use 2D reprojection for point cloud registration. Generally speaking, the scale of the point cloud used for registration is much larger than frame, which can be a model extracted from dozens of frames, or even a model of two rooms. For point cloud registration, there are also two different ways just the spase tracking and dense tracking in visual odometry: ICP based on dense points and Global Registration based on 3D features.","title":"Point Cloud Registration"},{"location":"modules/registration/#global-registration","text":"Global Registration does not require an initial pose, the basic steps are: - Extract 3D features - Feature matching - Outlier filtering - Calculate transformation matrix The methods are as following: std::shared_ptr<RegistrationResult> RansacRegistration(const geometry::PointCloud &source_pcd, const geometry::PointCloud &target_pcd, const RANSACParameter &r_para = RANSACParameter()); std::shared_ptr<RegistrationResult> RansacRegistration(const geometry::PointCloud &source_feature_pcd, const geometry::PointCloud &target_feature_pcd, const FeatureSet & source_features, const FeatureSet & target_features, const RANSACParameter &r_para); The difference between the above two functions is that the first is to directly register the two point clouds, and the second is to register the feature points, which means that the step of extracting 3D features is skipped. When building a system, saving the 3D features can avoid repeated calculation and improve efficiency. Extracting 3D features are generally implemented in libraries such as PCL or Open3D, but such libraries are too large. OpenCV does not implement 3D feature extraction, so FCLib implements this part. FPFH features can be calculated by the following function : std::tuple<geometry::PointCloud, FeatureSet> DownSampleAndExtractFeature(const geometry::PointCloud &pcd, const RANSACParameter &r_para = RANSACParameter()); The above function input is the point cloud and related parameters, and returns the extracted feature point position and features (33-dimensional vector). The we can perform knn search for the nearest point to get the matching result. Outlier filtering are quiet similar to the methods introduced in SparseTracking , so I won\u2019t introduce them here. Finally, the transformation matrix and inliers are calculated by RANSAC .","title":"Global Registration"},{"location":"modules/registration/#icp","text":"ICP (iterative closest point) is a very classic algorithm with many variants. FCLib implements two basic ICPs, one is point-to-point and the other is point-to-plane.","title":"ICP"},{"location":"modules/registration/#pointtopoint","text":"PointToPoint optimizes the distance from point to point. The optimized objective function is: \\(\\Vert T \\cdot p_s-p_t\\Vert^2\\) . Through OpenCV's KDtree to find the closest point, then the best transformation \\(T\\) can be obtained directly by the geometry::EstimateRigidTransformation (which can also be optimized by computing the Jacobian matrix). Then re-find the nearest point according to the new \\(T\\) , and iterate the process until convergence.","title":"PointToPoint"},{"location":"modules/registration/#pointtoplane","text":"PointToPlane optimizes the distance from point to plane, so it needs the vector normal of target point cloud. The equation of the plane whose normal vector is \\(n_t\\) and passing through \\(p_t\\) is $$ n_t^T (\\cdot p - p_t) = 0, $$ \\(n_t^T (\\cdot p - p_t)\\) is the distance from point to plane. So the objective function is: \\(\\Vert n_t^T\\cdot(T\\cdot p_s - p_t)\\Vert^2\\) . For the linearization of this cost function, you can view related_paper/lowk_point-to-plane_icp_techrep.pdf . The new \\(T\\) is obtained through computing Jacobian matrix and optimization, and then the closest point is re-finded according to the new \\(T\\) . Iterate this process until convergence . Both experimental and theoretical analysis prove that PointToPlane has a faster convergence rate than PointToPoint . The method is defined as following: std::shared_ptr<RegistrationResult> PointToPoint(const geometry::PointCloud &source, const geometry::PointCloud &target, const geometry::TransformationMatrix &init_T = geometry::TransformationMatrix::Identity(), const ICPParameter &icp_para = ICPParameter() ); std::shared_ptr<RegistrationResult> PointToPlane(const geometry::PointCloud &source, const geometry::PointCloud &target, const geometry::TransformationMatrix &init_T = geometry::TransformationMatrix::Identity(), const ICPParameter &icp_para = ICPParameter() ); icp_para are parameters of ICP, using the default value is a good choice. RegistrationResult is defined as following: class RegistrationResult { public: //relative transformation geometry::TransformationMatrix T; //the id of inliers geometry::FMatchSet correspondence_set_index; //inliers geometry::PointCorrespondenceSet correspondence_set; //rmse double rmse; }; In this case, it is left to the user to determine whether the registration is successful.","title":"PointToPlane"},{"location":"modules/tool/","text":"Other Useful Tools namespace: tool The tool module defines some of functions that other modules needs. At present, the tool contains basic image processing, control of console output color, Ply file reading and writing, timer, multithreading, etc. ConsoleColor Defines some controls for the console output color. The author hopes that the entire structure of FCLib is clear and the output is as elegant as possible. 'Red', 'yellow', 'green' and 'blue' are defined in the header file. Red corresponds to Error, yellow corresponds to Warning, and green means things is running as expected, the blue color is to output some related information, which is good for debug. After including the ConsoleColor header file, you can change the output color by following operation: //output error information std::cout<<RED<<\"[ERROR]::There is a error.\"<<RESET<<std::endl; RESET will set the output color to the default white color. ImageProcessing ImageProcessing.h implements some image processing related content, encapsulates some OpenCV functions, including Gaussian filter, Sobel filter, creation of image pyramid and so on. PLYManager PLYManager implements the reading and writing of ply files based on tinyply . In addition to vertices, colors, normals, and triangular faces, PLYManager also supports to read and write arbitrary elements with properties, and comments. For additional elements you want to add, they need to be packaged in AdditionalElement to be correctly used by PLYManager : struct AdditionalElement { //Number of elements size_t count; //The total number of bytes occupied by data, which is equal to (count * sizeof(type) * element_property.size()) size_t byte_size; //Name of elements std::string element_key; //name of elements' property std::vector<std::string> element_property; //type of property, which can be char, short, int, and unsigned version, float, double tinyply::Type type; //property can also be a list, this is the type of list's length tinyply::Type list_type = tinyply::Type::INVALID; //list'length size_t list_count = 0; //pointer to data unsigned char *data; }; //When reading additional elements, remember to delete the data pointer after conversion to the datatype you need. bool ReadPLY(const std::string &filename, geometry::Point3List &points, geometry::Point3List &normals, geometry::Point3List &colors, geometry::Point3uiList &triangles, std::vector<AdditionalElement> & additional_labels); bool WritePLY(const std::string &filename, const geometry::Point3List&points, const geometry::Point3List &normals, const geometry::Point3List &colors, const geometry::Point3uiList &triangles = geometry::Point3uiList(), const std::vector<std::string> & comments = std::vector<std::string>(), const std::vector<AdditionalElement> & additional_labels = std::vector<AdditionalElement>(), bool use_ascii = false); RPLYManager(Deleted) FCLib uses the RPLY library to read and write PLY and to read the point Cloud or triangle mesh. The loading and writing of 'geometry::PointCloud' and 'geometry::TriangleMesh' are encapsulation of this class. FCLib also supports reading or writing of other properties or labels of vertices(FCLib supports int32, float, double, unchar, ushort currently). However, when you want to do this, you need to call some low-level functions implemented in RPLYManager.h . //template function for reading the information of a certain label, you need to provide the label name, value type, and corresponding contained vector //You can read the values of one label at a time. template<typename T> bool ReadVertexLabelFromPLY(const std::string &filename, const std::string &label_name, std::vector<T> &vertex_labels); //Write additional label information into ply. It's a little troublesome to write. //You need to pack the information to be written into a container(vector), and the type of this vector is AdditionalLabel, which is a tuple type. It contains label_name, label_type, and a pointer to the data. //You write multiple attributes at once typedef std::tuple<std::string, e_ply_type, void *> AdditionalLabel; bool WritePLY(const std::string &filename, const geometry::Point3List&points, const geometry::Point3List &normals, const geometry::Point3List &colors, const std::vector<Eigen::Vector3i> &triangles, const std::vector<AdditionalLabel> & additional_labels = std::vector<AdditionalLabel>(), bool use_ascii = true); For reading and writing of labels or properties, you can check example/GetLabelUsingKDTree.cpp , which will perform reading and writing semantic information of meshes. Here FCLib can pack Reading operation into the same form as Writing, but in fact Writing is more troublesome to call, and it also means to allocate space for void pointers (which needs to be released). This is more dangerous behavior, so FCLib gave up this method. OBJManager FCLib uses OBJManager to read and write OBJ file, which is an encapsulation of tinyobjloader. MultiThreads What you need to provide is the parameters, that is, the input, and the output container that needs to be changed, and the function to call. The input parameters and the return are stored in two std::vector . The function prototype is as follows: template <class T1, class T2> void MultiThreads(const std::vector<T1> &p1, std::vector<T2> &p2, std::function<void(const T1 &, T2&)> &f); It is also very simple for to use this template. Put the input and output into two std::vectors , then use the std::function binding function, and pass them into the above functions together. The grid generation introduced later will have Examples of use. The implementation here is still too simple, which means that there can only be one input and one output. This template is temporarily written to meet a certain function for the author needs. MultiThreads does not care whether your loop can be parallelized, so it is the user's responsibility to ensure that the loop can be accelerated in parallel. TickTock TickTock is a timer, which is defined in class Timer . Timer can time multiple processes, as long as their names are different. Yes, to use Timer , you must define a unique name for the timing process. It is simple to use: tool::Timer timer; timer.TICK(\"My Process\"); // code of process timer.TOCK(\"My Process\"); timer.Log(\"My Process\"); Log is used to print the ellapsed time, The unit is milliseconds. You can use LogAll() to output all results. However, a warning will be output when 'Tick time' is larger than 'Tock time'. So try to use these tools correctly. OpenNIReader OpenNIReader encapsulates the interface of OpenNI to read depth and rgb, which needs to be improved. KeyframeBasedSlam A base class of the key frame-based slam system, example/BAFusion.cpp and example/FBAFusion.cpp are based on this class. CPPExtension CPPExtension.h implements some extensions of CPP. It currently includes the string slice operations Split and RSplit , and the template function ShuffleVector to randomly shuffle a std::vector . IO IO.h contains some IO operations of TUM dataset and Scannet dataset. AlignColorToDepth can be used to align color map and depth map, when the color camera and depth camera have different camera intrinsics( Generally, color camera has a high resolution).","title":"Other Useful Tools"},{"location":"modules/tool/#other-useful-tools","text":"namespace: tool The tool module defines some of functions that other modules needs. At present, the tool contains basic image processing, control of console output color, Ply file reading and writing, timer, multithreading, etc.","title":"Other Useful Tools"},{"location":"modules/tool/#consolecolor","text":"Defines some controls for the console output color. The author hopes that the entire structure of FCLib is clear and the output is as elegant as possible. 'Red', 'yellow', 'green' and 'blue' are defined in the header file. Red corresponds to Error, yellow corresponds to Warning, and green means things is running as expected, the blue color is to output some related information, which is good for debug. After including the ConsoleColor header file, you can change the output color by following operation: //output error information std::cout<<RED<<\"[ERROR]::There is a error.\"<<RESET<<std::endl; RESET will set the output color to the default white color.","title":"ConsoleColor"},{"location":"modules/tool/#imageprocessing","text":"ImageProcessing.h implements some image processing related content, encapsulates some OpenCV functions, including Gaussian filter, Sobel filter, creation of image pyramid and so on.","title":"ImageProcessing"},{"location":"modules/tool/#plymanager","text":"PLYManager implements the reading and writing of ply files based on tinyply . In addition to vertices, colors, normals, and triangular faces, PLYManager also supports to read and write arbitrary elements with properties, and comments. For additional elements you want to add, they need to be packaged in AdditionalElement to be correctly used by PLYManager : struct AdditionalElement { //Number of elements size_t count; //The total number of bytes occupied by data, which is equal to (count * sizeof(type) * element_property.size()) size_t byte_size; //Name of elements std::string element_key; //name of elements' property std::vector<std::string> element_property; //type of property, which can be char, short, int, and unsigned version, float, double tinyply::Type type; //property can also be a list, this is the type of list's length tinyply::Type list_type = tinyply::Type::INVALID; //list'length size_t list_count = 0; //pointer to data unsigned char *data; }; //When reading additional elements, remember to delete the data pointer after conversion to the datatype you need. bool ReadPLY(const std::string &filename, geometry::Point3List &points, geometry::Point3List &normals, geometry::Point3List &colors, geometry::Point3uiList &triangles, std::vector<AdditionalElement> & additional_labels); bool WritePLY(const std::string &filename, const geometry::Point3List&points, const geometry::Point3List &normals, const geometry::Point3List &colors, const geometry::Point3uiList &triangles = geometry::Point3uiList(), const std::vector<std::string> & comments = std::vector<std::string>(), const std::vector<AdditionalElement> & additional_labels = std::vector<AdditionalElement>(), bool use_ascii = false);","title":"PLYManager"},{"location":"modules/tool/#rplymanagerdeleted","text":"FCLib uses the RPLY library to read and write PLY and to read the point Cloud or triangle mesh. The loading and writing of 'geometry::PointCloud' and 'geometry::TriangleMesh' are encapsulation of this class. FCLib also supports reading or writing of other properties or labels of vertices(FCLib supports int32, float, double, unchar, ushort currently). However, when you want to do this, you need to call some low-level functions implemented in RPLYManager.h . //template function for reading the information of a certain label, you need to provide the label name, value type, and corresponding contained vector //You can read the values of one label at a time. template<typename T> bool ReadVertexLabelFromPLY(const std::string &filename, const std::string &label_name, std::vector<T> &vertex_labels); //Write additional label information into ply. It's a little troublesome to write. //You need to pack the information to be written into a container(vector), and the type of this vector is AdditionalLabel, which is a tuple type. It contains label_name, label_type, and a pointer to the data. //You write multiple attributes at once typedef std::tuple<std::string, e_ply_type, void *> AdditionalLabel; bool WritePLY(const std::string &filename, const geometry::Point3List&points, const geometry::Point3List &normals, const geometry::Point3List &colors, const std::vector<Eigen::Vector3i> &triangles, const std::vector<AdditionalLabel> & additional_labels = std::vector<AdditionalLabel>(), bool use_ascii = true); For reading and writing of labels or properties, you can check example/GetLabelUsingKDTree.cpp , which will perform reading and writing semantic information of meshes. Here FCLib can pack Reading operation into the same form as Writing, but in fact Writing is more troublesome to call, and it also means to allocate space for void pointers (which needs to be released). This is more dangerous behavior, so FCLib gave up this method.","title":"RPLYManager(Deleted)"},{"location":"modules/tool/#objmanager","text":"FCLib uses OBJManager to read and write OBJ file, which is an encapsulation of tinyobjloader.","title":"OBJManager"},{"location":"modules/tool/#multithreads","text":"What you need to provide is the parameters, that is, the input, and the output container that needs to be changed, and the function to call. The input parameters and the return are stored in two std::vector . The function prototype is as follows: template <class T1, class T2> void MultiThreads(const std::vector<T1> &p1, std::vector<T2> &p2, std::function<void(const T1 &, T2&)> &f); It is also very simple for to use this template. Put the input and output into two std::vectors , then use the std::function binding function, and pass them into the above functions together. The grid generation introduced later will have Examples of use. The implementation here is still too simple, which means that there can only be one input and one output. This template is temporarily written to meet a certain function for the author needs. MultiThreads does not care whether your loop can be parallelized, so it is the user's responsibility to ensure that the loop can be accelerated in parallel.","title":"MultiThreads"},{"location":"modules/tool/#ticktock","text":"TickTock is a timer, which is defined in class Timer . Timer can time multiple processes, as long as their names are different. Yes, to use Timer , you must define a unique name for the timing process. It is simple to use: tool::Timer timer; timer.TICK(\"My Process\"); // code of process timer.TOCK(\"My Process\"); timer.Log(\"My Process\"); Log is used to print the ellapsed time, The unit is milliseconds. You can use LogAll() to output all results. However, a warning will be output when 'Tick time' is larger than 'Tock time'. So try to use these tools correctly.","title":"TickTock"},{"location":"modules/tool/#opennireader","text":"OpenNIReader encapsulates the interface of OpenNI to read depth and rgb, which needs to be improved.","title":"OpenNIReader"},{"location":"modules/tool/#keyframebasedslam","text":"A base class of the key frame-based slam system, example/BAFusion.cpp and example/FBAFusion.cpp are based on this class.","title":"KeyframeBasedSlam"},{"location":"modules/tool/#cppextension","text":"CPPExtension.h implements some extensions of CPP. It currently includes the string slice operations Split and RSplit , and the template function ShuffleVector to randomly shuffle a std::vector .","title":"CPPExtension"},{"location":"modules/tool/#io","text":"IO.h contains some IO operations of TUM dataset and Scannet dataset. AlignColorToDepth can be used to align color map and depth map, when the color camera and depth camera have different camera intrinsics( Generally, color camera has a high resolution).","title":"IO"},{"location":"modules/visualization/","text":"Visualization namespace: visualization Visualization is for the convenience of observing the data. It always brings more intuitive understanding and feelings (for the researcher), and also makes the things you do look cooler (for the author). FCLib does not have much need for visualization. It is mainly used to display point clouds, triangular meshes (display colors, or phong models). FCLib also supports to display line segments (used to describe correspondence) and triangular pyramids (used to display camera positions) . The visualization is implemented in the class Visualizer , which is a partial encapsulation of Pangolin . The main member functions are as follows: class Visualizer { public: //initialize, name refer to the window name void Initialize(const std::string &name = \"FuckingCool\"); //insert a point coud into the buffer void AddPointCloud( const geometry::PointCloud &pcd); //insert a triangle mesh into the buffer void AddTriangleMesh(const geometry::TriangleMesh &mesh); //Display once, mainly used to display dynamically changing objects, such as the reconstruction process void ShowOnce(); //Continuous display, during which the geometric objects in the buffer cannot be changed void Show(); //Add a set of Camera (triangular pyramid), the first parameter is the poses, the second parameter is the camera colors void AddCameraSet(const geometry::SE3List & _camera_poses, const geometry::Point3List &_camera_colors); //Add a set of line segments, the first parameter is the two end points of the line, and the second parameter is the color of the line segment void AddLineSet(const geometry::Point3List & _points, const std::vector<std::pair<int, int>> &_index, const geometry::Point3List &_line_colors); }; visualizer does not support the display of multiple kinds of geometric objects(such as display of point cloud and mesh), nor does it support UI. These functions may not be supported in the future.","title":"Visualization"},{"location":"modules/visualization/#visualization","text":"namespace: visualization Visualization is for the convenience of observing the data. It always brings more intuitive understanding and feelings (for the researcher), and also makes the things you do look cooler (for the author). FCLib does not have much need for visualization. It is mainly used to display point clouds, triangular meshes (display colors, or phong models). FCLib also supports to display line segments (used to describe correspondence) and triangular pyramids (used to display camera positions) . The visualization is implemented in the class Visualizer , which is a partial encapsulation of Pangolin . The main member functions are as follows: class Visualizer { public: //initialize, name refer to the window name void Initialize(const std::string &name = \"FuckingCool\"); //insert a point coud into the buffer void AddPointCloud( const geometry::PointCloud &pcd); //insert a triangle mesh into the buffer void AddTriangleMesh(const geometry::TriangleMesh &mesh); //Display once, mainly used to display dynamically changing objects, such as the reconstruction process void ShowOnce(); //Continuous display, during which the geometric objects in the buffer cannot be changed void Show(); //Add a set of Camera (triangular pyramid), the first parameter is the poses, the second parameter is the camera colors void AddCameraSet(const geometry::SE3List & _camera_poses, const geometry::Point3List &_camera_colors); //Add a set of line segments, the first parameter is the two end points of the line, and the second parameter is the color of the line segment void AddLineSet(const geometry::Point3List & _points, const std::vector<std::pair<int, int>> &_index, const geometry::Point3List &_line_colors); }; visualizer does not support the display of multiple kinds of geometric objects(such as display of point cloud and mesh), nor does it support UI. These functions may not be supported in the future.","title":"Visualization"}]}